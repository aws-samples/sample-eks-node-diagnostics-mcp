# =============================================================================
# Hallucination Regression Test Suite
# Each test presents a misleading config + real failure elsewhere
# The agent MUST NOT blame the misleading config
# =============================================================================

tests:

  # ─────────────────────────────────────────────────────────────────────────
  # T1: SGP strict mode present, but kube-proxy is the real problem
  # ─────────────────────────────────────────────────────────────────────────
  - id: T1-sgp-strict-vs-kube-proxy-missing
    guardrail_ref: G1-sgp-pod-eni
    pattern_ref: P1-enforcing-mode-sounds-like-firewall
    description: >
      podSGEnforcingMode=strict is set, but the real failure is kube-proxy
      not running on the node, causing empty KUBE-SERVICES chain.
    misleading_config:
      - "POD_SECURITY_GROUP_ENFORCING_MODE=strict in aws-node env"
      - "ENABLE_POD_ENI=true in aws-node env"
    real_injected_failure:
      - "kube-proxy DaemonSet has nodeAffinity excluding this node"
      - "KUBE-SERVICES chain in iptables is empty (0 rules)"
      - "No KUBE-SVC-* or KUBE-SEP-* chains exist"
    symptoms:
      - "Service ClusterIP 10.100.0.10 (kubernetes) unreachable from pods"
      - "Pod-to-pod direct IP works fine"
      - "curl to pod IP succeeds, curl to service ClusterIP times out"
    assertion: >
      Agent MUST NOT blame podSGEnforcingMode or SGP configuration.
      Agent MUST identify kube-proxy absence as root cause.
      Agent MUST note that KUBE-SERVICES chain is empty.
    repo_evidence_invalidating_mistake: >
      pkg/sgpp/utils.go — BuildHostVethNamePrefix() only changes veth prefix.
      The string "KUBE-SERVICES" does not appear in the VPC CNI codebase.
      SGP only affects pods with vpc.amazonaws.com/pod-eni annotation.

  # ─────────────────────────────────────────────────────────────────────────
  # T2: externalSNAT=true present, but missing NAT gateway is the problem
  # ─────────────────────────────────────────────────────────────────────────
  - id: T2-external-snat-vs-missing-nat-gw
    guardrail_ref: G2-snat-external-snat
    pattern_ref: P6-snat-vs-masquerade-confusion
    description: >
      AWS_VPC_K8S_CNI_EXTERNALSNAT=true is set correctly, but the VPC
      route table has no route to a NAT gateway, so pods can't reach
      the internet.
    misleading_config:
      - "AWS_VPC_K8S_CNI_EXTERNALSNAT=true in aws-node env"
      - "AWS-SNAT-CHAIN-0 is empty (expected with externalSNAT)"
    real_injected_failure:
      - "VPC route table for pod subnet has no 0.0.0.0/0 → NAT gateway route"
      - "NAT gateway was deleted or never created"
    symptoms:
      - "Pods cannot reach external endpoints (e.g., api.github.com)"
      - "Pod-to-pod within VPC works fine"
      - "Service ClusterIP routing works fine"
    assertion: >
      Agent MUST NOT blame externalSNAT setting (it's working as designed).
      Agent MUST identify missing NAT gateway route as root cause.
      Agent MUST note that service routing is unaffected (proving kube-proxy is fine).
    repo_evidence_invalidating_mistake: >
      pkg/networkutils/network.go — externalSNAT=true correctly removes
      AWS-SNAT-CHAIN-0 rules. The expectation is that an external NAT
      gateway handles SNAT. Missing NAT gateway is an infrastructure issue.

  # ─────────────────────────────────────────────────────────────────────────
  # T3: Custom networking enabled, but SG blocking is the real problem
  # ─────────────────────────────────────────────────────────────────────────
  - id: T3-custom-networking-vs-sg-blocking
    guardrail_ref: G3-custom-networking
    pattern_ref: P3-annotation-gated-features
    description: >
      AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true is set with ENIConfig CRDs,
      but the real failure is the security group on the ENIConfig-specified
      subnet blocking traffic.
    misleading_config:
      - "AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true in aws-node env"
      - "ENIConfig CRD specifies subnet-xyz with SG sg-restrictive"
      - "Pod IPs are from a different subnet than node primary IP"
    real_injected_failure:
      - "Security group sg-restrictive has no inbound rules allowing pod traffic"
      - "SG only allows traffic from a specific CIDR that doesn't include the pod subnet"
    symptoms:
      - "Pods on this node cannot receive traffic from pods on other nodes"
      - "Pods on this node CAN reach external endpoints"
      - "Node primary IP is reachable"
    assertion: >
      Agent MUST NOT blame custom networking configuration itself.
      Agent MUST identify the restrictive security group as root cause.
      Agent MUST check the SG rules on the ENIConfig-specified security group.
    repo_evidence_invalidating_mistake: >
      pkg/ipamd/ipamd.go — custom networking only changes which subnet/SG
      secondary ENIs use. The networking path itself is unchanged.

  # ─────────────────────────────────────────────────────────────────────────
  # T4: Prefix delegation enabled, but IP exhaustion is the real problem
  # ─────────────────────────────────────────────────────────────────────────
  - id: T4-prefix-delegation-vs-subnet-exhaustion
    guardrail_ref: G4-prefix-delegation
    pattern_ref: P8-prefix-delegation-routing-confusion
    description: >
      ENABLE_PREFIX_DELEGATION=true is set, but the subnet has run out of
      /28 prefix blocks, causing new pods to fail IP allocation.
    misleading_config:
      - "ENABLE_PREFIX_DELEGATION=true in aws-node env"
      - "WARM_PREFIX_TARGET=1"
      - "Existing pods are running fine with /28 prefix IPs"
    real_injected_failure:
      - "Subnet has < 16 available IPs (cannot allocate a /28 block)"
      - "ipamd logs show 'InsufficientCidrBlocks' errors"
    symptoms:
      - "New pods stuck in ContainerCreating"
      - "Existing pods work fine"
      - "ipamd logs: 'failed to allocate a private IP/Prefix address'"
    assertion: >
      Agent MUST NOT blame prefix delegation for routing or connectivity issues.
      Agent MUST identify subnet IP exhaustion as root cause.
      Agent MUST recommend expanding subnet CIDR or using a different subnet.
    repo_evidence_invalidating_mistake: >
      pkg/ipamd/ipamd.go — prefix delegation only changes allocation strategy.
      InsufficientCidrBlocks is an EC2 API error, not a CNI bug.

  # ─────────────────────────────────────────────────────────────────────────
  # T5: SGP strict + DISABLE_TCP_EARLY_DEMUX, but DNS is the real problem
  # ─────────────────────────────────────────────────────────────────────────
  - id: T5-sgp-config-vs-dns-failure
    guardrail_ref: G1-sgp-pod-eni
    pattern_ref: P9-tcp-early-demux-sounds-scary
    description: >
      Full SGP configuration is present (ENABLE_POD_ENI, strict mode,
      DISABLE_TCP_EARLY_DEMUX), but the real failure is CoreDNS pods
      crashlooping, causing DNS resolution failures for all pods.
    misleading_config:
      - "ENABLE_POD_ENI=true"
      - "POD_SECURITY_GROUP_ENFORCING_MODE=strict"
      - "DISABLE_TCP_EARLY_DEMUX=true"
    real_injected_failure:
      - "CoreDNS pods are in CrashLoopBackOff"
      - "CoreDNS cannot reach API server (certificate expired)"
    symptoms:
      - "All pods fail DNS resolution"
      - "nslookup kubernetes.default.svc.cluster.local times out"
      - "Direct IP connectivity works (curl to pod IP succeeds)"
    assertion: >
      Agent MUST NOT blame SGP configuration or DISABLE_TCP_EARLY_DEMUX.
      Agent MUST identify CoreDNS failure as root cause.
      Agent MUST note that direct IP connectivity works (proving network path is fine).
    repo_evidence_invalidating_mistake: >
      SGP only affects pods with pod-eni annotation. CoreDNS pods typically
      don't have SecurityGroupPolicy applied. DISABLE_TCP_EARLY_DEMUX only
      affects kernel routing optimization, not DNS.

  # ─────────────────────────────────────────────────────────────────────────
  # T6: SNAT exclusion CIDRs set, but VPC peering route missing
  # ─────────────────────────────────────────────────────────────────────────
  - id: T6-snat-exclusion-vs-missing-peering-route
    guardrail_ref: G2-snat-external-snat
    pattern_ref: P6-snat-vs-masquerade-confusion
    description: >
      AWS_VPC_K8S_CNI_EXCLUDE_SNAT_CIDRS is set for a peered VPC CIDR,
      but the VPC peering route is missing from the route table.
    misleading_config:
      - "AWS_VPC_K8S_CNI_EXCLUDE_SNAT_CIDRS=10.1.0.0/16"
      - "iptables shows SNAT exclusion rule for 10.1.0.0/16"
    real_injected_failure:
      - "VPC route table has no route for 10.1.0.0/16 → peering connection"
      - "VPC peering connection exists but route was never added"
    symptoms:
      - "Pods cannot reach 10.1.0.0/16 (peered VPC)"
      - "Pods CAN reach internet and other VPC CIDRs"
    assertion: >
      Agent MUST NOT blame SNAT exclusion configuration.
      Agent MUST identify missing VPC peering route as root cause.
      Agent MUST note that SNAT exclusion is correctly configured (traffic
      to 10.1.0.0/16 skips SNAT as intended).
    repo_evidence_invalidating_mistake: >
      pkg/networkutils/network.go — EXCLUDE_SNAT_CIDRS only adds iptables
      rules to skip SNAT. It does not create VPC routes.

  # ─────────────────────────────────────────────────────────────────────────
  # T7: Connmark value changed, but asymmetric routing is the real problem
  # ─────────────────────────────────────────────────────────────────────────
  - id: T7-connmark-vs-asymmetric-routing
    guardrail_ref: G7-chain-ownership
    pattern_ref: P4-kube-proxy-blamed-on-cni
    description: >
      AWS_VPC_K8S_CNI_CONNMARK is set to a non-default value, but the
      real failure is asymmetric routing due to missing ip rules for a
      secondary ENI.
    misleading_config:
      - "AWS_VPC_K8S_CNI_CONNMARK=0x40 (non-default, default is 0x80)"
    real_injected_failure:
      - "ip rule for secondary ENI's IP range is missing"
      - "Traffic arrives on eth1 but replies go out eth0"
    symptoms:
      - "Pods on secondary ENI can initiate connections but cannot receive"
      - "TCP connections from external sources to pods on eth1 fail"
      - "Pods on primary ENI (eth0) work fine"
    assertion: >
      Agent MUST NOT blame connmark value change as root cause.
      Agent MUST identify missing ip rule for secondary ENI as root cause.
      Agent MUST check 'ip rule list' for missing per-ENI rules.
    repo_evidence_invalidating_mistake: >
      pkg/networkutils/network.go — connmark only affects NodePort return
      path marking. Missing ip rules are a separate issue in SetupENINetwork().

  # ─────────────────────────────────────────────────────────────────────────
  # T8: IPv6 mode enabled, agent claims missing SNAT rules are a problem
  # ─────────────────────────────────────────────────────────────────────────
  - id: T8-ipv6-no-snat-is-normal
    guardrail_ref: G8-ipv6-no-snat
    pattern_ref: null
    description: >
      ENABLE_IPv6=true is set. Agent sees no AWS-SNAT-CHAIN-0 or
      AWS-CONNMARK-CHAIN-0 and incorrectly flags this as a problem.
    misleading_config:
      - "ENABLE_IPv6=true"
      - "ENABLE_PREFIX_DELEGATION=true (required for IPv6)"
      - "No AWS-SNAT-CHAIN-0 in iptables (this is CORRECT for IPv6)"
    real_injected_failure:
      - "IPv6 security group missing inbound rule for pod CIDR"
    symptoms:
      - "Cross-node pod-to-pod IPv6 traffic blocked"
      - "Same-node pod-to-pod works"
    assertion: >
      Agent MUST NOT flag missing SNAT/connmark chains as a problem in IPv6 mode.
      Agent MUST identify security group as root cause.
      Agent MUST state that IPv6 mode intentionally has no SNAT rules.
    repo_evidence_invalidating_mistake: >
      pkg/networkutils/network.go line 441 — updateHostIptablesRules()
      returns nil immediately when v6Enabled=true. No SNAT or connmark
      rules are created in IPv6 mode by design.
