---
# =============================================================================
# NODE-LEVEL FAULT INJECTION FOR MCP TOOL TESTING
# =============================================================================
# These issues generate entries in kubelet journal, dmesg, syslog, containerd,
# IPAMD, CNI, iptables, and kernel logs — the files the MCP tools actually parse.
#
# Target nodes (first 6 in cluster):
#   Node 1 (i-0f2c36ed3c6441131): OOM + memory pressure + kernel kills
#   Node 2 (i-07b1807d7f5b67fd2): Volume mount failures + CSI errors
#   Node 3 (i-066b6285d77f63954): CNI/IPAMD stress + IP exhaustion signals
#   Node 4 (i-0cdd62ca87b93aa86): Image pull failures + registry errors
#   Node 5 (i-021c69a6c01051590): DNS failures + network timeouts
#   Node 6 (i-0441d2643ac9718f8): Probe failures + CrashLoop + scheduling
# =============================================================================

apiVersion: v1
kind: Namespace
metadata:
  name: node-fault-injection

---
# =============================================================================
# NODE 1: OOM KILLS + MEMORY PRESSURE (i-0f2c36ed3c6441131)
# Triggers: kernel OOM killer, Memory cgroup out of memory, exit code 137
# Detected by: errors, search, summarize, correlate
# Log files: dmesg, kubelet journal, syslog
# =============================================================================

# Heavy OOM — allocates until killed by kernel OOM killer
apiVersion: apps/v1
kind: Deployment
metadata:
  name: oom-heavy
  namespace: node-fault-injection
  labels:
    fault-target: node-1-oom
spec:
  replicas: 4
  selector:
    matchLabels:
      app: oom-heavy
  template:
    metadata:
      labels:
        app: oom-heavy
    spec:
      nodeName: ip-172-31-0-114.us-west-2.compute.internal
      containers:
      - name: oom
        image: public.ecr.aws/docker/library/python:3.11-slim
        command: ["python3", "-c", "import time; chunks=[]; \nwhile True:\n    chunks.append(bytearray(50*1024*1024))\n    time.sleep(0.1)\n"]
        resources:
          requests:
            cpu: 10m
            memory: 32Mi
          limits:
            memory: 64Mi
      - name: oom-fast
        image: public.ecr.aws/docker/library/python:3.11-slim
        command: ["python3", "-c", "x=[bytearray(10*1024*1024) for _ in range(100)]"]
        resources:
          requests:
            cpu: 10m
            memory: 16Mi
          limits:
            memory: 32Mi

---
# Fork bomb style — triggers "fork/exec resource temporarily unavailable" in kubelet
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pid-pressure
  namespace: node-fault-injection
  labels:
    fault-target: node-1-pid
spec:
  replicas: 3
  selector:
    matchLabels:
      app: pid-pressure
  template:
    metadata:
      labels:
        app: pid-pressure
    spec:
      nodeName: ip-172-31-0-114.us-west-2.compute.internal
      containers:
      - name: threads
        image: public.ecr.aws/docker/library/python:3.11-slim
        command: ["python3", "-c", "import threading, time\ndef spin():\n    while True: time.sleep(1)\nfor i in range(200):\n    threading.Thread(target=spin, daemon=True).start()\ntime.sleep(3600)\n"]
        resources:
          requests:
            cpu: 10m
            memory: 64Mi
          limits:
            memory: 128Mi

---
# =============================================================================
# NODE 2: VOLUME MOUNT FAILURES (i-07b1807d7f5b67fd2)
# Triggers: MountVolume.SetUp failed, Unable to attach or mount volumes,
#           FailedMount, timed out waiting for volume, PVC pending
# Detected by: errors, search, network_diagnostics (routes/eni), summarize
# Log files: kubelet journal
# =============================================================================

# Non-existent PVC — kubelet logs FailedMount + MountVolume.SetUp failed
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nonexistent-ebs-vol
  namespace: node-fault-injection
spec:
  accessModes: [ReadWriteOnce]
  storageClassName: gp2-does-not-exist
  resources:
    requests:
      storage: 50Gi

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mount-fail-ebs
  namespace: node-fault-injection
  labels:
    fault-target: node-2-mount
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mount-fail-ebs
  template:
    metadata:
      labels:
        app: mount-fail-ebs
    spec:
      nodeName: ip-172-31-0-203.us-west-2.compute.internal
      containers:
      - name: app
        image: public.ecr.aws/docker/library/busybox:latest
        command: ["sleep", "3600"]
        volumeMounts:
        - name: data
          mountPath: /data
        resources:
          requests:
            cpu: 10m
            memory: 16Mi
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: nonexistent-ebs-vol

---
# NFS mount timeout — triggers mount.nfs timed out in kubelet/dmesg
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mount-fail-nfs
  namespace: node-fault-injection
  labels:
    fault-target: node-2-nfs
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mount-fail-nfs
  template:
    metadata:
      labels:
        app: mount-fail-nfs
    spec:
      nodeName: ip-172-31-0-203.us-west-2.compute.internal
      containers:
      - name: app
        image: public.ecr.aws/docker/library/busybox:latest
        command: ["sleep", "3600"]
        volumeMounts:
        - name: nfs-vol
          mountPath: /mnt/nfs
        resources:
          requests:
            cpu: 10m
            memory: 16Mi
      volumes:
      - name: nfs-vol
        nfs:
          server: 10.99.99.99
          path: /nonexistent/share

---
# Missing secret mount — triggers CreateContainerConfigError + "secrets not found"
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mount-fail-secret
  namespace: node-fault-injection
  labels:
    fault-target: node-2-secret
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mount-fail-secret
  template:
    metadata:
      labels:
        app: mount-fail-secret
    spec:
      nodeName: ip-172-31-0-203.us-west-2.compute.internal
      containers:
      - name: app
        image: public.ecr.aws/docker/library/busybox:latest
        command: ["sleep", "3600"]
        env:
        - name: DB_PASS
          valueFrom:
            secretKeyRef:
              name: prod-database-creds
              key: password
        - name: API_KEY
          valueFrom:
            secretKeyRef:
              name: external-api-secret
              key: token
        resources:
          requests:
            cpu: 10m
            memory: 16Mi

---
# =============================================================================
# NODE 3: CNI/IPAMD STRESS (i-066b6285d77f63954)
# Triggers: IP allocation pressure, ENI attachment churn, aws-node errors
# Detected by: network_diagnostics (cni, ipamd, eni sections), errors, search
# Log files: IPAMD/aws-node logs, CNI config, kubelet journal
# =============================================================================

# Many pods to exhaust secondary IPs on the node's ENIs
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ip-exhaust
  namespace: node-fault-injection
  labels:
    fault-target: node-3-cni
spec:
  replicas: 30
  selector:
    matchLabels:
      app: ip-exhaust
  template:
    metadata:
      labels:
        app: ip-exhaust
    spec:
      nodeName: ip-172-31-0-241.us-west-2.compute.internal
      containers:
      - name: sleeper
        image: public.ecr.aws/docker/library/busybox:latest
        command: ["sleep", "3600"]
        resources:
          requests:
            cpu: 1m
            memory: 4Mi
          limits:
            memory: 8Mi
      terminationGracePeriodSeconds: 0

---
# Pods that do heavy networking — generates conntrack entries + iptables activity
apiVersion: apps/v1
kind: Deployment
metadata:
  name: conntrack-pressure
  namespace: node-fault-injection
  labels:
    fault-target: node-3-conntrack
spec:
  replicas: 5
  selector:
    matchLabels:
      app: conntrack-pressure
  template:
    metadata:
      labels:
        app: conntrack-pressure
    spec:
      nodeName: ip-172-31-0-241.us-west-2.compute.internal
      containers:
      - name: netspam
        image: public.ecr.aws/docker/library/busybox:latest
        command: ["sh", "-c", "while true; do for i in $(seq 1 50); do wget -q -T 1 -O /dev/null http://10.100.0.1:443 2>&1 || true & done; wait; sleep 2; done"]
        resources:
          requests:
            cpu: 10m
            memory: 16Mi
          limits:
            memory: 32Mi

---
# Pod that tries to use hostNetwork — generates CNI-related kubelet logs
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cni-conflict
  namespace: node-fault-injection
  labels:
    fault-target: node-3-cni-conflict
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cni-conflict
  template:
    metadata:
      labels:
        app: cni-conflict
    spec:
      nodeName: ip-172-31-0-241.us-west-2.compute.internal
      hostNetwork: true
      containers:
      - name: listener
        image: public.ecr.aws/docker/library/busybox:latest
        # Try to bind to a port that's likely in use — generates "address already in use"
        command: ["sh", "-c", "while true; do nc -l -p 10250 2>&1 || true; sleep 5; done"]
        resources:
          requests:
            cpu: 5m
            memory: 8Mi

---
# =============================================================================
# NODE 4: IMAGE PULL FAILURES (i-0cdd62ca87b93aa86)
# Triggers: Failed to pull image, unauthorized, manifest not found,
#           repository does not exist, ImagePullBackOff, ErrImagePull
# Detected by: errors, search, summarize
# Log files: kubelet journal, containerd logs
# =============================================================================

# Non-existent ECR repo
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bad-ecr-image
  namespace: node-fault-injection
  labels:
    fault-target: node-4-image
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bad-ecr-image
  template:
    metadata:
      labels:
        app: bad-ecr-image
    spec:
      nodeName: ip-172-31-0-30.us-west-2.compute.internal
      containers:
      - name: app
        image: 466162272783.dkr.ecr.us-west-2.amazonaws.com/totally-fake-repo:v1.0.0
        resources:
          requests:
            cpu: 10m
            memory: 16Mi

---
# Non-existent Docker Hub image — "manifest not found"
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bad-dockerhub-image
  namespace: node-fault-injection
  labels:
    fault-target: node-4-image
spec:
  replicas: 2
  selector:
    matchLabels:
      app: bad-dockerhub-image
  template:
    metadata:
      labels:
        app: bad-dockerhub-image
    spec:
      nodeName: ip-172-31-0-30.us-west-2.compute.internal
      containers:
      - name: app
        image: public.ecr.aws/docker/library/busybox:nonexistent-tag-v999
        resources:
          requests:
            cpu: 10m
            memory: 16Mi

---
# Wrong architecture image — triggers "exec format error"
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wrong-arch-image
  namespace: node-fault-injection
  labels:
    fault-target: node-4-arch
spec:
  replicas: 2
  selector:
    matchLabels:
      app: wrong-arch-image
  template:
    metadata:
      labels:
        app: wrong-arch-image
    spec:
      nodeName: ip-172-31-0-30.us-west-2.compute.internal
      containers:
      - name: app
        image: public.ecr.aws/docker/library/alpine:latest
        command: ["/bin/nonexistent-binary-that-does-not-exist"]
        resources:
          requests:
            cpu: 10m
            memory: 16Mi

---
# Private registry without auth — "unauthorized: authentication required"
apiVersion: apps/v1
kind: Deployment
metadata:
  name: private-registry-noauth
  namespace: node-fault-injection
  labels:
    fault-target: node-4-auth
spec:
  replicas: 2
  selector:
    matchLabels:
      app: private-registry-noauth
  template:
    metadata:
      labels:
        app: private-registry-noauth
    spec:
      nodeName: ip-172-31-0-30.us-west-2.compute.internal
      containers:
      - name: app
        image: ghcr.io/this-org-does-not-exist/fake-private-image:latest
        resources:
          requests:
            cpu: 10m
            memory: 16Mi

---
# =============================================================================
# NODE 5: DNS + NETWORK FAILURES (i-021c69a6c01051590)
# Triggers: DNS timeout, NXDOMAIN, SERVFAIL, dial tcp i/o timeout,
#           connection refused, network is unreachable, TLS handshake timeout
# Detected by: errors, search, network_diagnostics (dns section), correlate
# Log files: kubelet journal, coredns logs, pod logs, syslog
# =============================================================================

# Aggressive DNS failure generator — floods CoreDNS with NXDOMAIN
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dns-flood
  namespace: node-fault-injection
  labels:
    fault-target: node-5-dns
spec:
  replicas: 5
  selector:
    matchLabels:
      app: dns-flood
  template:
    metadata:
      labels:
        app: dns-flood
    spec:
      nodeName: ip-172-31-1-145.us-west-2.compute.internal
      containers:
      - name: dns-spam
        image: public.ecr.aws/docker/library/busybox:latest
        command: ["sh", "-c", "while true; do nslookup nonexistent-svc-abc.default.svc.cluster.local 2>&1; nslookup fake-external-api.company.internal 2>&1; nslookup this.domain.definitely.does.not.exist.xyz 2>&1; sleep 1; done"]
        resources:
          requests:
            cpu: 5m
            memory: 8Mi

---
# Network timeout generator — dial tcp timeouts + connection refused
apiVersion: apps/v1
kind: Deployment
metadata:
  name: network-timeout
  namespace: node-fault-injection
  labels:
    fault-target: node-5-network
spec:
  replicas: 3
  selector:
    matchLabels:
      app: network-timeout
  template:
    metadata:
      labels:
        app: network-timeout
    spec:
      nodeName: ip-172-31-1-145.us-west-2.compute.internal
      containers:
      - name: timeout-gen
        image: public.ecr.aws/docker/library/busybox:latest
        command: ["sh", "-c", "while true; do wget -q -T 3 -O /dev/null http://10.99.99.99:8080 2>&1 || true; wget -q -T 3 -O /dev/null http://192.168.255.255:443 2>&1 || true; wget -q -T 2 -O /dev/null https://api.nonexistent-service.internal:443 2>&1 || true; sleep 2; done"]
        resources:
          requests:
            cpu: 5m
            memory: 8Mi

---
# TLS handshake failure generator
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tls-failures
  namespace: node-fault-injection
  labels:
    fault-target: node-5-tls
spec:
  replicas: 2
  selector:
    matchLabels:
      app: tls-failures
  template:
    metadata:
      labels:
        app: tls-failures
    spec:
      nodeName: ip-172-31-1-145.us-west-2.compute.internal
      containers:
      - name: tls-fail
        image: public.ecr.aws/docker/library/busybox:latest
        command: ["sh", "-c", "while true; do wget -q -T 3 -O /dev/null https://self-signed.badssl.com/ 2>&1 || true; wget -q -T 3 -O /dev/null https://expired.badssl.com/ 2>&1 || true; sleep 3; done"]
        resources:
          requests:
            cpu: 5m
            memory: 8Mi

---
# =============================================================================
# NODE 6: PROBE FAILURES + CRASHLOOP + SCHEDULING (i-0441d2643ac9718f8)
# Triggers: Readiness/Liveness probe failed, CrashLoopBackOff,
#           Back-off restarting failed container, PLEG ContainerDied
# Detected by: errors, search, summarize, correlate
# Log files: kubelet journal, containerd logs
# =============================================================================

# Liveness + readiness probe failures — kubelet logs these directly
apiVersion: apps/v1
kind: Deployment
metadata:
  name: probe-fail-http
  namespace: node-fault-injection
  labels:
    fault-target: node-6-probes
spec:
  replicas: 4
  selector:
    matchLabels:
      app: probe-fail-http
  template:
    metadata:
      labels:
        app: probe-fail-http
    spec:
      nodeName: ip-172-31-1-189.us-west-2.compute.internal
      containers:
      - name: app
        image: public.ecr.aws/docker/library/busybox:latest
        command: ["sh", "-c", "sleep 3600"]
        readinessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 2
          periodSeconds: 3
          failureThreshold: 3
        livenessProbe:
          httpGet:
            path: /live
            port: 8080
          initialDelaySeconds: 15
          periodSeconds: 5
          failureThreshold: 3
        resources:
          requests:
            cpu: 10m
            memory: 16Mi

---
# Rapid CrashLoop — generates PLEG ContainerDied + Back-off restarting
apiVersion: apps/v1
kind: Deployment
metadata:
  name: crashloop-rapid
  namespace: node-fault-injection
  labels:
    fault-target: node-6-crash
spec:
  replicas: 5
  selector:
    matchLabels:
      app: crashloop-rapid
  template:
    metadata:
      labels:
        app: crashloop-rapid
    spec:
      nodeName: ip-172-31-1-189.us-west-2.compute.internal
      containers:
      - name: crasher
        image: public.ecr.aws/docker/library/busybox:latest
        command: ["sh", "-c", "echo 'FATAL: segfault at 0x0000 in /usr/bin/app'; exit 139"]
        resources:
          requests:
            cpu: 5m
            memory: 8Mi

---
# Container that exits with different error codes — generates varied kubelet errors
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multi-exit-codes
  namespace: node-fault-injection
  labels:
    fault-target: node-6-exits
spec:
  replicas: 3
  selector:
    matchLabels:
      app: multi-exit-codes
  template:
    metadata:
      labels:
        app: multi-exit-codes
    spec:
      nodeName: ip-172-31-1-189.us-west-2.compute.internal
      containers:
      - name: sigkill
        image: public.ecr.aws/docker/library/busybox:latest
        command: ["sh", "-c", "kill -9 $$"]
        resources:
          requests:
            cpu: 5m
            memory: 8Mi
      - name: sigterm
        image: public.ecr.aws/docker/library/busybox:latest
        command: ["sh", "-c", "kill -15 $$"]
        resources:
          requests:
            cpu: 5m
            memory: 8Mi
      - name: exit1
        image: public.ecr.aws/docker/library/busybox:latest
        command: ["sh", "-c", "echo 'permission denied: /var/run/secrets'; exit 1"]
        resources:
          requests:
            cpu: 5m
            memory: 8Mi

---
# =============================================================================
# CROSS-NODE: Scheduling failures (no nodeName — stays Pending)
# Triggers: FailedScheduling, Insufficient cpu/memory, node selector mismatch,
#           taint/toleration mismatch, Unschedulable
# Detected by: cluster_health (shows pending pods), batch_collect
# =============================================================================

# Impossible resource request
apiVersion: apps/v1
kind: Deployment
metadata:
  name: unschedulable-giant
  namespace: node-fault-injection
  labels:
    fault-target: scheduling
spec:
  replicas: 2
  selector:
    matchLabels:
      app: unschedulable-giant
  template:
    metadata:
      labels:
        app: unschedulable-giant
    spec:
      containers:
      - name: hungry
        image: public.ecr.aws/docker/library/busybox:latest
        command: ["sleep", "3600"]
        resources:
          requests:
            cpu: "96"
            memory: 256Gi

---
# GPU node selector — no GPU nodes exist
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-mismatch
  namespace: node-fault-injection
  labels:
    fault-target: scheduling
spec:
  replicas: 2
  selector:
    matchLabels:
      app: gpu-mismatch
  template:
    metadata:
      labels:
        app: gpu-mismatch
    spec:
      nodeSelector:
        nvidia.com/gpu.present: "true"
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
      containers:
      - name: app
        image: public.ecr.aws/docker/library/busybox:latest
        command: ["sleep", "3600"]
        resources:
          requests:
            cpu: 10m
            memory: 16Mi
