import * as cdk from 'aws-cdk-lib';
import * as s3 from 'aws-cdk-lib/aws-s3';
import * as s3deploy from 'aws-cdk-lib/aws-s3-deployment';
import * as lambda from 'aws-cdk-lib/aws-lambda';
import * as iam from 'aws-cdk-lib/aws-iam';
import * as cognito from 'aws-cdk-lib/aws-cognito';
import * as s3n from 'aws-cdk-lib/aws-s3-notifications';
import * as cr from 'aws-cdk-lib/custom-resources';
import * as kms from 'aws-cdk-lib/aws-kms';
import * as logs from 'aws-cdk-lib/aws-logs';
import { Construct } from 'constructs';
import * as path from 'path';

export interface SsmAutomationGatewayV2Props {
  /**
   * Name for the AgentCore Gateway
   * @default 'EksNodeLogMcpGW'
   */
  readonly gatewayName?: string;

  /**
   * Name for the Cognito User Pool
   * @default 'ssm-automation-gateway-pool'
   */
  readonly cognitoUserPoolName?: string;

  /**
   * Name for the Cognito Resource Server
   * @default 'ssm-automation-gateway-id'
   */
  readonly resourceServerName?: string;

  /**
   * Number of days to retain logs in S3
   * @default 1
   */
  readonly logRetentionDays?: number;

  /**
   * Enable KMS encryption for S3 bucket
   * @default true
   */
  readonly enableEncryption?: boolean;

  /**
   * Enable CloudTrail data events for S3
   * @default false (requires CloudTrail to be configured separately)
   */
  readonly enableS3DataEvents?: boolean;

  /**
   * Name for the SOP S3 bucket. If not provided, a default name is generated.
   * @default '{stackName}-sops-{accountId}'
   */
  readonly sopBucketName?: string;
}

/**
 * Production-grade MCP Server for EKS Node Log Collection
 * 
 * Features:
 * - Async task pattern with idempotency
 * - Byte-range streaming for multi-GB files
 * - Manifest validation and completeness verification
 * - Pre-indexed error findings
 * - Secure artifact references with presigned URLs
 * - KMS encryption at rest
 * - Comprehensive audit logging
 */
export class SsmAutomationGatewayV2Construct extends Construct {
  public readonly logsBucket: s3.Bucket;
  public readonly ssmAutomationFunction: lambda.Function;
  public readonly unzipFunction: lambda.Function;
  public readonly findingsIndexerFunction: lambda.Function;
  public readonly userPool: cognito.UserPool;
  public readonly userPoolClient: cognito.UserPoolClient;
  public readonly ssmAutomationRole: iam.Role;
  public readonly gatewayExecutionRole: iam.Role;
  public readonly encryptionKey?: kms.Key;
  public readonly sopBucket: s3.Bucket;

  constructor(scope: Construct, id: string, props: SsmAutomationGatewayV2Props = {}) {
    super(scope, id);

    const gatewayName = props.gatewayName ?? 'EksNodeLogMcpGW';
    const cognitoUserPoolName = props.cognitoUserPoolName ?? 'ssm-automation-gateway-pool';
    const resourceServerName = props.resourceServerName ?? 'ssm-automation-gateway-id';
    const logRetentionDays = props.logRetentionDays ?? 1;
    const enableEncryption = props.enableEncryption ?? true;

    // ========================================================================
    // KMS ENCRYPTION KEY (if enabled)
    // ========================================================================

    if (enableEncryption) {
      this.encryptionKey = new kms.Key(this, 'LogsEncryptionKey', {
        alias: `${cdk.Stack.of(this).stackName}-logs-key`,
        description: 'KMS key for EKS node log encryption',
        enableKeyRotation: true,
        removalPolicy: cdk.RemovalPolicy.DESTROY,
      });
    }

    // ========================================================================
    // IAM ROLES
    // ========================================================================

    // Role for SSM Automation to assume
    this.ssmAutomationRole = new iam.Role(this, 'SSMAutomationRole', {
      roleName: `${cdk.Stack.of(this).stackName}-ssm-automation-role`,
      assumedBy: new iam.ServicePrincipal('ssm.amazonaws.com'),
    });

    // SSM Automation permissions
    this.ssmAutomationRole.addToPolicy(new iam.PolicyStatement({
      sid: 'SSMAutomationPermissions',
      effect: iam.Effect.ALLOW,
      actions: [
        'ssm:StartAutomationExecution',
        'ssm:StopAutomationExecution',
        'ssm:GetAutomationExecution',
        'ssm:DescribeAutomationExecutions',
        'ssm:DescribeAutomationStepExecutions',
        'ssm:SendCommand',
        'ssm:GetCommandInvocation',
        'ssm:ListCommandInvocations',
        'ssm:ListCommands',
        'ssm:CancelCommand',
        'ssm:GetDocument',
        'ssm:DescribeDocument',
        'ssm:GetParameters',
        'ssm:GetParameter',
        'ssm:DescribeInstanceInformation',
        'ssm:GetConnectionStatus',
      ],
      resources: ['*'],
    }));

    // EKS and EC2 permissions for log collection
    this.ssmAutomationRole.addToPolicy(new iam.PolicyStatement({
      sid: 'EKSAndEC2Permissions',
      effect: iam.Effect.ALLOW,
      actions: [
        'ec2:DescribeInstances',
        'ec2:DescribeTags',
        'ec2:DescribeInstanceStatus',
        'eks:DescribeCluster',
        'eks:ListClusters',
      ],
      resources: ['*'],
    }));

    // ========================================================================
    // S3 BUCKET - Log Storage with Encryption
    // ========================================================================

    const bucketProps: s3.BucketProps = {
      bucketName: `${cdk.Stack.of(this).stackName.toLowerCase()}-logs-${cdk.Stack.of(this).account}`,
      versioned: true,
      blockPublicAccess: s3.BlockPublicAccess.BLOCK_ALL,
      enforceSSL: true,
      lifecycleRules: [
        {
          id: 'DeleteOldLogs',
          enabled: true,
          expiration: cdk.Duration.days(logRetentionDays),
        },
        {
          id: 'DeleteOldIdempotencyMappings',
          enabled: true,
          prefix: 'idempotency/',
          expiration: cdk.Duration.days(7),
        },
        {
          id: 'DeleteOldExecutionRegionMappings',
          enabled: true,
          prefix: 'execution-regions/',
          expiration: cdk.Duration.days(7),
        },
        {
          id: 'ExpireOldBaselines',
          enabled: true,
          prefix: 'baselines/',
          expiration: cdk.Duration.days(90),
        },
      ],
      removalPolicy: cdk.RemovalPolicy.DESTROY,
      autoDeleteObjects: true,
    };

    // Add encryption if enabled
    if (this.encryptionKey) {
      Object.assign(bucketProps, {
        encryption: s3.BucketEncryption.KMS,
        encryptionKey: this.encryptionKey,
        bucketKeyEnabled: true,
      });
    } else {
      Object.assign(bucketProps, {
        encryption: s3.BucketEncryption.S3_MANAGED,
      });
    }

    this.logsBucket = new s3.Bucket(this, 'LogsBucket', bucketProps);

    // Grant SSM Automation role access to the bucket
    this.logsBucket.grantReadWrite(this.ssmAutomationRole);
    if (this.encryptionKey) {
      this.encryptionKey.grantEncryptDecrypt(this.ssmAutomationRole);

      // Allow any principal in this account to use the key for S3 uploads
      // This is needed for EC2 instance roles (EKS worker nodes) to upload
      // tcpdump captures and other artifacts via `aws s3 cp`
      // IMPORTANT: Using AnyPrincipal with account condition (not AccountRootPrincipal)
      // because AccountRootPrincipal delegates to IAM policies, and worker node roles
      // typically don't have KMS identity-based policies. AnyPrincipal in a key policy
      // grants direct access without requiring an identity-based policy.
      this.encryptionKey.addToResourcePolicy(new iam.PolicyStatement({
        sid: 'AllowAccountPrincipalsEncrypt',
        effect: iam.Effect.ALLOW,
        principals: [new iam.AnyPrincipal()],
        actions: ['kms:GenerateDataKey', 'kms:GenerateDataKey*', 'kms:Encrypt', 'kms:Decrypt', 'kms:DescribeKey', 'kms:ReEncrypt*'],
        resources: ['*'],
        conditions: {
          StringEquals: {
            'aws:PrincipalAccount': cdk.Stack.of(this).account,
          },
        },
      }));
    }

    // Bucket policy to allow EC2 instances in the account to upload logs
    // Bucket policy to allow EC2 instances in the account to upload logs
    this.logsBucket.addToResourcePolicy(new iam.PolicyStatement({
      sid: 'AllowEC2InstancesUpload',
      effect: iam.Effect.ALLOW,
      principals: [new iam.AccountRootPrincipal()],
      actions: ['s3:PutObject', 's3:GetBucketPolicyStatus', 's3:GetBucketAcl'],
      resources: [this.logsBucket.bucketArn, `${this.logsBucket.bucketArn}/*`],
      conditions: {
        StringEquals: {
          'aws:PrincipalAccount': cdk.Stack.of(this).account,
        },
      },
    }));

    // ========================================================================
    // S3 BUCKET - SOP Storage
    // ========================================================================

    this.sopBucket = new s3.Bucket(this, 'SOPBucket', {
      bucketName: props.sopBucketName ?? `${cdk.Stack.of(this).stackName.toLowerCase()}-sops-${cdk.Stack.of(this).account}`,
      versioned: true,
      blockPublicAccess: s3.BlockPublicAccess.BLOCK_ALL,
      enforceSSL: true,
      removalPolicy: cdk.RemovalPolicy.DESTROY,
      autoDeleteObjects: true,
    });

    // Auto-deploy runbooks from local sops/runbooks/ to S3 on every cdk deploy
    new s3deploy.BucketDeployment(this, 'RunbookDeployment', {
      sources: [s3deploy.Source.asset(path.join(__dirname, '..', 'sops', 'runbooks'))],
      destinationBucket: this.sopBucket,
      destinationKeyPrefix: 'runbooks/',
      prune: true, // Remove S3 objects not in the local source
      memoryLimit: 256,
    });

    // ========================================================================
    // UNZIP LAMBDA FUNCTION
    // ========================================================================

    const unzipLambdaRole = new iam.Role(this, 'UnzipLambdaRole', {
      roleName: `${cdk.Stack.of(this).stackName}-unzip-lambda-role`,
      assumedBy: new iam.ServicePrincipal('lambda.amazonaws.com'),
      managedPolicies: [
        iam.ManagedPolicy.fromAwsManagedPolicyName('service-role/AWSLambdaBasicExecutionRole'),
      ],
    });

    this.logsBucket.grantReadWrite(unzipLambdaRole);
    if (this.encryptionKey) {
      this.encryptionKey.grantEncryptDecrypt(unzipLambdaRole);
    }

    // Create findings indexer first so we can reference it
    const findingsIndexerRole = new iam.Role(this, 'FindingsIndexerRole', {
      roleName: `${cdk.Stack.of(this).stackName}-findings-indexer-role`,
      assumedBy: new iam.ServicePrincipal('lambda.amazonaws.com'),
      managedPolicies: [
        iam.ManagedPolicy.fromAwsManagedPolicyName('service-role/AWSLambdaBasicExecutionRole'),
      ],
    });

    this.logsBucket.grantReadWrite(findingsIndexerRole);
    if (this.encryptionKey) {
      this.encryptionKey.grantEncryptDecrypt(findingsIndexerRole);
    }

    this.findingsIndexerFunction = new lambda.Function(this, 'FindingsIndexerFunction', {
      functionName: `${cdk.Stack.of(this).stackName}-findings-indexer`,
      runtime: lambda.Runtime.PYTHON_3_11,
      handler: 'index.lambda_handler',
      role: findingsIndexerRole,
      timeout: cdk.Duration.minutes(5),
      memorySize: 1024,
      environment: {
        LOGS_BUCKET_NAME: this.logsBucket.bucketName,
      },
      code: lambda.Code.fromInline(this.getFindingsIndexerCode()),
      logRetention: logs.RetentionDays.TWO_WEEKS,
    });

    // Now create unzip function with reference to findings indexer
    this.unzipFunction = new lambda.Function(this, 'UnzipFunction', {
      functionName: `${cdk.Stack.of(this).stackName}-unzip-function`,
      runtime: lambda.Runtime.PYTHON_3_11,
      handler: 'index.lambda_handler',
      role: unzipLambdaRole,
      timeout: cdk.Duration.minutes(5),
      memorySize: 1024,
      environment: {
        FINDINGS_INDEXER_FUNCTION: this.findingsIndexerFunction.functionName,
      },
      code: lambda.Code.fromInline(this.getUnzipLambdaCode()),
      logRetention: logs.RetentionDays.TWO_WEEKS,
    });

    // Grant unzip function permission to invoke findings indexer
    this.findingsIndexerFunction.grantInvoke(unzipLambdaRole);

    // Add S3 notification to trigger unzip on archive uploads
    this.logsBucket.addEventNotification(
      s3.EventType.OBJECT_CREATED,
      new s3n.LambdaDestination(this.unzipFunction),
      { suffix: '.zip' }
    );
    this.logsBucket.addEventNotification(
      s3.EventType.OBJECT_CREATED,
      new s3n.LambdaDestination(this.unzipFunction),
      { suffix: '.tar.gz' }
    );

    // ========================================================================
    // SSM AUTOMATION LAMBDA FUNCTION (Enhanced)
    // ========================================================================

    const lambdaExecutionRole = new iam.Role(this, 'LambdaExecutionRole', {
      roleName: `${cdk.Stack.of(this).stackName}-lambda-execution-role`,
      assumedBy: new iam.ServicePrincipal('lambda.amazonaws.com'),
      managedPolicies: [
        iam.ManagedPolicy.fromAwsManagedPolicyName('service-role/AWSLambdaBasicExecutionRole'),
      ],
    });

    // SSM Automation access
    lambdaExecutionRole.addToPolicy(new iam.PolicyStatement({
      sid: 'SSMAutomationAccess',
      effect: iam.Effect.ALLOW,
      actions: [
        'ssm:StartAutomationExecution',
        'ssm:GetAutomationExecution',
        'ssm:DescribeAutomationExecutions',
        'ssm:StopAutomationExecution',
        'ssm:DescribeInstanceInformation',
        'ssm:SendCommand',
        'ssm:GetCommandInvocation',
        'ssm:ListCommands',
        'ssm:ListCommandInvocations',
      ],
      resources: ['*'],
    }));

    // SSM Document access (cross-region support)
    lambdaExecutionRole.addToPolicy(new iam.PolicyStatement({
      sid: 'SSMDocumentAccess',
      effect: iam.Effect.ALLOW,
      actions: ['ssm:GetDocument', 'ssm:DescribeDocument'],
      resources: [
        `arn:aws:ssm:*::document/AWSSupport-CollectEKSInstanceLogs`,
        `arn:aws:ssm:*:${cdk.Stack.of(this).account}:document/*`,
      ],
    }));

    // EC2 and EKS permissions for cross-region detection and cluster_health
    lambdaExecutionRole.addToPolicy(new iam.PolicyStatement({
      sid: 'EC2AndEKSDescribe',
      effect: iam.Effect.ALLOW,
      actions: [
        'ec2:DescribeInstances',
        'ec2:DescribeRegions',
        'ec2:DescribeNetworkInterfaces',
        'ec2:DescribeSubnets',
        'ec2:DescribeSecurityGroups',
        'ec2:DescribeRouteTables',
        'eks:DescribeCluster',
        'eks:ListClusters',
        'eks:ListNodegroups',
        'eks:DescribeNodegroup',
        'ssm:DescribeInstanceInformation',
        'autoscaling:DescribeAutoScalingGroups',
      ],
      resources: ['*'],
    }));

    // S3 access
    this.logsBucket.grantReadWrite(lambdaExecutionRole);
    if (this.encryptionKey) {
      this.encryptionKey.grantEncryptDecrypt(lambdaExecutionRole);
    }

    // PassRole for SSM
    lambdaExecutionRole.addToPolicy(new iam.PolicyStatement({
      sid: 'PassRoleForSSM',
      effect: iam.Effect.ALLOW,
      actions: ['iam:PassRole'],
      resources: [this.ssmAutomationRole.roleArn],
      conditions: {
        StringEquals: {
          'iam:PassedToService': 'ssm.amazonaws.com',
        },
      },
    }));

    // SOP bucket read access (for list_sops / get_sop)
    this.sopBucket.grantRead(lambdaExecutionRole);

    this.ssmAutomationFunction = new lambda.Function(this, 'SSMAutomationFunction', {
      functionName: `${cdk.Stack.of(this).stackName}-ssm-automation`,
      runtime: lambda.Runtime.PYTHON_3_11,
      handler: 'ssm-automation-enhanced.lambda_handler',
      role: lambdaExecutionRole,
      timeout: cdk.Duration.minutes(5),
      memorySize: 1024,
      environment: {
        LOGS_BUCKET_NAME: this.logsBucket.bucketName,
        SSM_AUTOMATION_ROLE_ARN: this.ssmAutomationRole.roleArn,
        SOP_BUCKET_NAME: this.sopBucket.bucketName,
      },
      code: lambda.Code.fromAsset(path.join(__dirname, 'lambda')),
      logRetention: logs.RetentionDays.TWO_WEEKS,
    });

    // ========================================================================
    // GATEWAY EXECUTION ROLE
    // ========================================================================

    this.gatewayExecutionRole = new iam.Role(this, 'GatewayExecutionRole', {
      roleName: `${cdk.Stack.of(this).stackName}-gateway-execution-role`,
      assumedBy: new iam.ServicePrincipal('bedrock-agentcore.amazonaws.com'),
    });

    this.gatewayExecutionRole.addToPolicy(new iam.PolicyStatement({
      effect: iam.Effect.ALLOW,
      actions: ['lambda:InvokeFunction'],
      resources: [this.ssmAutomationFunction.functionArn],
    }));

    this.gatewayExecutionRole.addToPolicy(new iam.PolicyStatement({
      effect: iam.Effect.ALLOW,
      actions: ['logs:CreateLogGroup', 'logs:CreateLogStream', 'logs:PutLogEvents'],
      resources: ['*'],
    }));

    // Add Lambda resource-based policy to allow AgentCore to invoke
    this.ssmAutomationFunction.addPermission('AllowAgentCoreInvoke', {
      principal: new iam.ServicePrincipal('bedrock-agentcore.amazonaws.com'),
      action: 'lambda:InvokeFunction',
      sourceAccount: cdk.Stack.of(this).account,
    });

    // ========================================================================
    // COGNITO - Authentication
    // ========================================================================

    this.userPool = new cognito.UserPool(this, 'UserPool', {
      userPoolName: cognitoUserPoolName,
      passwordPolicy: {
        minLength: 8,
        requireUppercase: false,
        requireLowercase: false,
        requireDigits: false,
        requireSymbols: false,
      },
      removalPolicy: cdk.RemovalPolicy.DESTROY,
    });

    const resourceServer = new cognito.UserPoolResourceServer(this, 'ResourceServer', {
      userPool: this.userPool,
      identifier: resourceServerName,
      scopes: [
        { scopeName: 'gateway:read', scopeDescription: 'Read access to gateway' },
        { scopeName: 'gateway:write', scopeDescription: 'Write access to gateway' },
      ],
    });

    this.userPoolClient = new cognito.UserPoolClient(this, 'UserPoolClient', {
      userPool: this.userPool,
      userPoolClientName: 'ssm-automation-gateway-client',
      generateSecret: true,
      oAuth: {
        flows: { clientCredentials: true },
        scopes: [
          cognito.OAuthScope.custom(`${resourceServerName}/gateway:read`),
          cognito.OAuthScope.custom(`${resourceServerName}/gateway:write`),
        ],
      },
      supportedIdentityProviders: [cognito.UserPoolClientIdentityProvider.COGNITO],
    });
    this.userPoolClient.node.addDependency(resourceServer);

    const userPoolDomain = new cognito.UserPoolDomain(this, 'UserPoolDomain', {
      userPool: this.userPool,
      cognitoDomain: {
        domainPrefix: `${cdk.Stack.of(this).stackName.toLowerCase()}-${cdk.Stack.of(this).account}`,
      },
    });

    // ========================================================================
    // CLIENT SECRET RETRIEVAL - Custom Resource
    // ========================================================================

    const clientSecretRetrieverRole = new iam.Role(this, 'ClientSecretRetrieverRole', {
      roleName: `${cdk.Stack.of(this).stackName}-secret-retriever-role`,
      assumedBy: new iam.ServicePrincipal('lambda.amazonaws.com'),
      managedPolicies: [
        iam.ManagedPolicy.fromAwsManagedPolicyName('service-role/AWSLambdaBasicExecutionRole'),
      ],
    });

    clientSecretRetrieverRole.addToPolicy(new iam.PolicyStatement({
      effect: iam.Effect.ALLOW,
      actions: ['cognito-idp:DescribeUserPoolClient'],
      resources: [this.userPool.userPoolArn],
    }));

    const clientSecretRetrieverFunction = new lambda.Function(this, 'ClientSecretRetrieverFunction', {
      functionName: `${cdk.Stack.of(this).stackName}-secret-retriever`,
      runtime: lambda.Runtime.PYTHON_3_11,
      handler: 'index.handler',
      role: clientSecretRetrieverRole,
      timeout: cdk.Duration.seconds(30),
      code: lambda.Code.fromInline(this.getClientSecretRetrieverCode()),
    });

    const clientSecretRetriever = new cr.AwsCustomResource(this, 'ClientSecretRetriever', {
      onCreate: {
        service: 'Lambda',
        action: 'invoke',
        parameters: {
          FunctionName: clientSecretRetrieverFunction.functionName,
          Payload: JSON.stringify({
            RequestType: 'Create',
            ResourceProperties: {
              UserPoolId: this.userPool.userPoolId,
              ClientId: this.userPoolClient.userPoolClientId,
            },
          }),
        },
        physicalResourceId: cr.PhysicalResourceId.of('ClientSecretRetriever'),
      },
      policy: cr.AwsCustomResourcePolicy.fromStatements([
        new iam.PolicyStatement({
          actions: ['lambda:InvokeFunction'],
          resources: [clientSecretRetrieverFunction.functionArn],
        }),
      ]),
    });
    clientSecretRetriever.node.addDependency(this.userPoolClient);

    // ========================================================================
    // AGENTCORE GATEWAY
    // ========================================================================

    const gateway = new cdk.CfnResource(this, 'AgentCoreGateway', {
      type: 'AWS::BedrockAgentCore::Gateway',
      properties: {
        Name: gatewayName,
        Description: 'Production-grade EKS Node Log MCP Server with byte-range streaming and pre-indexed findings',
        ProtocolType: 'MCP',
        AuthorizerType: 'CUSTOM_JWT',
        AuthorizerConfiguration: {
          CustomJWTAuthorizer: {
            AllowedClients: [this.userPoolClient.userPoolClientId],
            DiscoveryUrl: `https://cognito-idp.${cdk.Stack.of(this).region}.amazonaws.com/${this.userPool.userPoolId}/.well-known/openid-configuration`,
          },
        },
        RoleArn: this.gatewayExecutionRole.roleArn,
      },
    });
    gateway.node.addDependency(this.userPoolClient);
    gateway.node.addDependency(userPoolDomain);

    // Gateway Target for Lambda
    const gatewayTarget = new cdk.CfnResource(this, 'LambdaGatewayTarget', {
      type: 'AWS::BedrockAgentCore::GatewayTarget',
      properties: {
        GatewayIdentifier: gateway.ref,
        Name: 'NodeLog',
        Description: 'Enhanced EKS Node Log Collection Target',
        TargetConfiguration: {
          Mcp: {
            Lambda: {
              LambdaArn: this.ssmAutomationFunction.functionArn,
              ToolSchema: {
                InlinePayload: this.getToolSchemaDefinitions(),
              },
            },
          },
        },
        CredentialProviderConfigurations: [
          { CredentialProviderType: 'GATEWAY_IAM_ROLE' },
        ],
      },
    });
    gatewayTarget.node.addDependency(this.ssmAutomationFunction);
    gatewayTarget.node.addDependency(this.gatewayExecutionRole);

    // ========================================================================
    // OUTPUTS
    // ========================================================================

    new cdk.CfnOutput(this, 'GatewayId', {
      description: 'ID of the AgentCore Gateway',
      value: gateway.ref,
      exportName: `${cdk.Stack.of(this).stackName}-GatewayId`,
    });

    new cdk.CfnOutput(this, 'GatewayUrl', {
      description: 'MCP Server URL',
      value: gateway.getAtt('GatewayUrl').toString(),
      exportName: `${cdk.Stack.of(this).stackName}-GatewayUrl`,
    });

    new cdk.CfnOutput(this, 'CognitoUserPoolId', {
      description: 'Cognito User Pool ID',
      value: this.userPool.userPoolId,
      exportName: `${cdk.Stack.of(this).stackName}-CognitoUserPoolId`,
    });

    new cdk.CfnOutput(this, 'CognitoClientId', {
      description: 'OAuth Client ID',
      value: this.userPoolClient.userPoolClientId,
      exportName: `${cdk.Stack.of(this).stackName}-CognitoClientId`,
    });

    new cdk.CfnOutput(this, 'OAuthExchangeUrl', {
      description: 'OAuth Token URL',
      value: `https://${cdk.Stack.of(this).stackName.toLowerCase()}-${cdk.Stack.of(this).account}.auth.${cdk.Stack.of(this).region}.amazoncognito.com/oauth2/token`,
      exportName: `${cdk.Stack.of(this).stackName}-OAuthExchangeUrl`,
    });

    new cdk.CfnOutput(this, 'OAuthScope', {
      description: 'OAuth Scope',
      value: `${resourceServerName}/gateway:read`,
      exportName: `${cdk.Stack.of(this).stackName}-OAuthScope`,
    });

    new cdk.CfnOutput(this, 'LogsBucketName', {
      description: 'S3 bucket for collected logs',
      value: this.logsBucket.bucketName,
      exportName: `${cdk.Stack.of(this).stackName}-LogsBucketName`,
    });

    new cdk.CfnOutput(this, 'SSMAutomationRoleArn', {
      description: 'SSM Automation Role ARN',
      value: this.ssmAutomationRole.roleArn,
      exportName: `${cdk.Stack.of(this).stackName}-SSMAutomationRoleArn`,
    });

    if (this.encryptionKey) {
      new cdk.CfnOutput(this, 'EncryptionKeyArn', {
        description: 'KMS Encryption Key ARN',
        value: this.encryptionKey.keyArn,
        exportName: `${cdk.Stack.of(this).stackName}-EncryptionKeyArn`,
      });
    }

    new cdk.CfnOutput(this, 'SOPBucketName', {
      description: 'S3 bucket for Standard Operating Procedures',
      value: this.sopBucket.bucketName,
      exportName: `${cdk.Stack.of(this).stackName}-SOPBucketName`,
    });
  }


  /**
   * Returns the enhanced tool schema definitions for the MCP Gateway
   */
  private getToolSchemaDefinitions(): object[] {
    return [
      // =====================================================================
      // TIER 1: CORE OPERATIONS
      // =====================================================================
      {
        Name: 'collect',
        Description: 'Start EKS log collection from a worker node. Returns immediately with executionId for async polling. Recommended workflow: collect → status (poll until complete) → quick_triage (ONE call for full analysis). Alternative detailed workflow: collect → status → validate → errors → summarize. Do NOT read individual files unless quick_triage/summarize/network_diagnostics are insufficient. Supports cross-region: auto-detects instance region or accepts explicit region parameter. CITATION: When presenting results, always cite the executionId and region returned.',
        InputSchema: {
          Type: 'object',
          Properties: {
            instanceId: {
              Type: 'string',
              Description: 'The EC2 instance ID of the EKS worker node (e.g., i-0123456789abcdef0)',
            },
            idempotencyToken: {
              Type: 'string',
              Description: 'Optional token to prevent duplicate executions. If provided and a matching execution exists, returns the existing executionId.',
            },
            region: {
              Type: 'string',
              Description: 'AWS region where the instance runs (e.g., us-west-2). Optional: auto-detected from instance if omitted.',
            },
          },
          Required: ['instanceId'],
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            executionId: { Type: 'string', Description: 'SSM Automation execution ID for polling' },
            instanceId: { Type: 'string' },
            region: { Type: 'string' },
            status: { Type: 'string' },
            estimatedCompletionTime: { Type: 'string' },
            idempotent: { Type: 'boolean', Description: 'True if returning existing execution' },
            task: {
              Type: 'object',
              Description: 'Async task envelope for polling',
              Properties: {
                taskId: { Type: 'string', Description: 'Same as executionId — use with status tool' },
                state: { Type: 'string', Description: 'running|completed|failed|cancelled' },
                message: { Type: 'string' },
                progress: { Type: 'integer', Description: '0-100 percent' },
              },
            },
          },
        },
      },
      {
        Name: 'status',
        Description: 'Get detailed status of a log collection execution including progress percentage, step details, and failure reasons. Automatically resolves the region where the execution was started. CITATION: Always cite executionId and status in your response.',
        InputSchema: {
          Type: 'object',
          Properties: {
            executionId: {
              Type: 'string',
              Description: 'The SSM Automation execution ID returned from collect',
            },
            includeStepDetails: {
              Type: 'boolean',
              Description: 'Include individual step status (default: true)',
            },
            region: {
              Type: 'string',
              Description: 'AWS region of the execution. Optional: auto-resolved from stored execution metadata if omitted.',
            },
          },
          Required: ['executionId'],
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            automation: {
              Type: 'object',
              Properties: {
                executionId: { Type: 'string' },
                status: { Type: 'string', Description: 'InProgress|Success|Failed|Cancelled' },
                progress: { Type: 'integer', Description: '0-100 percent' },
                failureReason: { Type: 'string' },
                stepDetails: { Type: 'array' },
              },
            },
            task: {
              Type: 'object',
              Description: 'Normalized task state for async polling',
              Properties: {
                taskId: { Type: 'string' },
                state: { Type: 'string', Description: 'running|completed|failed|cancelled' },
                message: { Type: 'string' },
                progress: { Type: 'integer', Description: '0-100 percent' },
              },
            },
          },
        },
      },
      {
        Name: 'validate',
        Description: 'Verify all expected files were extracted from the log bundle. Returns manifest with file counts, sizes, and missing patterns. After validate succeeds, call errors (not read) to get findings, then summarize for root cause analysis. CITATION: Cite fileCount, totalSizeHuman, and any missingPatterns.',
        InputSchema: {
          Type: 'object',
          Properties: {
            executionId: {
              Type: 'string',
              Description: 'SSM execution ID to validate',
            },
            instanceId: {
              Type: 'string',
              Description: 'Alternative: Instance ID to locate bundle',
            },
          },
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            complete: { Type: 'boolean' },
            fileCount: { Type: 'integer' },
            totalSize: { Type: 'integer' },
            totalSizeHuman: { Type: 'string' },
            missingPatterns: { Type: 'array' },
            foundPatterns: { Type: 'array' },
            hasFindingsIndex: { Type: 'boolean' },
            manifest: { Type: 'array', Description: 'File list with key, fullKey, size, sizeHuman' },
          },
        },
      },
      {
        Name: 'errors',
        Description: 'Get pre-indexed error findings (fast path). Returns categorized errors by severity with finding_ids for citation. Each finding has a stable finding_id (F-001 format). IMPORTANT: After calling errors, pass the finding_ids to the summarize tool for root cause analysis and remediation — do NOT manually read individual log files. CITATION: Always cite finding_id and severity when referencing findings.',
        InputSchema: {
          Type: 'object',
          Properties: {
            instanceId: {
              Type: 'string',
              Description: 'The EC2 instance ID to get error summary for',
            },
            severity: {
              Type: 'string',
              Description: 'Filter by severity: critical, high, medium, low, info, or all (default: all). Legacy "warning" maps to "high".',
            },
            response_format: {
              Type: 'string',
              Description: '"concise" (default) returns finding_id/severity/pattern/file/count only. "detailed" includes full sample text and line numbers.',
            },
            pageSize: {
              Type: 'integer',
              Description: 'Number of findings per page (default: 50, max: 200)',
            },
            pageToken: {
              Type: 'string',
              Description: 'Opaque token for next page (from previous response nextPageToken)',
            },
            clusterContext: {
              Type: 'string',
              Description: 'EKS cluster name for baseline subtraction. When provided, findings seen 10+ times are annotated as baseline (normal operation).',
            },
            incident_time: {
              Type: 'string',
              Description: 'ISO8601 timestamp of the incident (e.g., 2026-02-13T09:10:00Z). Analysis will be restricted to +/- 5 minutes around this time. If omitted and no start_time/end_time, defaults to last 10 minutes.',
            },
            start_time: {
              Type: 'string',
              Description: 'Start of analysis window (ISO8601). Use with end_time for explicit time range.',
            },
            end_time: {
              Type: 'string',
              Description: 'End of analysis window (ISO8601). Use with start_time for explicit time range.',
            },
          },
          Required: ['instanceId'],
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            instanceId: { Type: 'string' },
            findings: { Type: 'array', Description: 'Array of findings with finding_id, severity, pattern, file, count' },
            totalFindings: { Type: 'integer' },
            summary: { Type: 'object', Description: 'Counts by severity: critical, high, medium, low, info' },
            hasMore: { Type: 'boolean' },
            nextPageToken: { Type: 'string' },
            coverage_report: { Type: 'object', Description: 'files_scanned, files_skipped, scan_complete' },
            window_start_utc: { Type: 'string', Description: 'UTC start of analysis window (ISO8601)' },
            window_end_utc: { Type: 'string', Description: 'UTC end of analysis window (ISO8601)' },
            resolution_reason: { Type: 'string', Description: 'How the time window was determined' },
            time_window_filter: { Type: 'object', Description: 'excluded_outside_window, unparseable_timestamps counts' },
          },
        },
      },
      {
        Name: 'read',
        Description: 'Read a chunk of a log file using byte-range streaming. NO TRUNCATION. Use ONLY when you need raw log content that is not available from errors, summarize, search, or network_diagnostics tools. Prefer higher-level tools first — they are faster and avoid timeouts. CITATION: Cite logKey, startByte, endByte, and totalSize.',
        InputSchema: {
          Type: 'object',
          Properties: {
            logKey: {
              Type: 'string',
              Description: 'The S3 key of the log file (from validate manifest)',
            },
            startByte: {
              Type: 'integer',
              Description: 'Starting byte offset (default: 0). Snaps forward to next newline.',
            },
            endByte: {
              Type: 'integer',
              Description: 'Ending byte offset (default: startByte + 1MB). Snaps forward to next newline.',
            },
            startLine: {
              Type: 'integer',
              Description: 'Alternative: Starting line number (1-based)',
            },
            lineCount: {
              Type: 'integer',
              Description: 'Number of lines to return when using startLine (default: 1000)',
            },
          },
          Required: ['logKey'],
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            logKey: { Type: 'string' },
            content: { Type: 'string' },
            startByte: { Type: 'integer', Description: 'Actual start byte (line-aligned)' },
            endByte: { Type: 'integer', Description: 'Actual end byte (line-aligned)' },
            totalSize: { Type: 'integer' },
            hasMore: { Type: 'boolean' },
            nextChunkToken: { Type: 'string', Description: 'Pass as startByte for next chunk' },
            truncated: { Type: 'boolean', Description: 'Always false — never truncates' },
            lineAligned: { Type: 'boolean', Description: 'True when byte reads are snapped to newline boundaries' },
          },
        },
      },

      // =====================================================================
      // TIER 2: ADVANCED ANALYSIS
      // =====================================================================
      {
        Name: 'search',
        Description: 'Full-text regex search across logs. ONLY use this if quick_triage topEvidence was insufficient and you need to find a SPECIFIC pattern not already surfaced. Do NOT use search to re-examine findings already shown in quick_triage. CITATION: Cite finding_id (S-NNN format), file name, and match count.',
        InputSchema: {
          Type: 'object',
          Properties: {
            instanceId: {
              Type: 'string',
              Description: 'The EC2 instance ID to search logs for',
            },
            query: {
              Type: 'string',
              Description: 'Regex pattern to search for (e.g., "OOMKilled|MemoryPressure")',
            },
            logTypes: {
              Type: 'string',
              Description: 'Comma-separated log types to search. Available types: kubelet, containerd, docker, dmesg, kernel, messages, system, security, networking, storage, ipamd, pods, aws-node, coredns, nodeadm, sandbox-image, eks-agents, config, modinfo, sysctls, gpu, multus, soci-snapshotter, throttling (default: all)',
            },
            maxResults: {
              Type: 'integer',
              Description: 'Maximum results per file (default: 100, max: 500)',
            },
            response_format: {
              Type: 'string',
              Description: '"concise" (default) or "detailed" — controls verbosity of match context',
            },
            incident_time: {
              Type: 'string',
              Description: 'ISO8601 timestamp of the incident. Search results will be filtered to +/- 5 minutes around this time. If omitted and no start_time/end_time, defaults to last 10 minutes.',
            },
            start_time: {
              Type: 'string',
              Description: 'Start of analysis window (ISO8601). Use with end_time for explicit time range.',
            },
            end_time: {
              Type: 'string',
              Description: 'End of analysis window (ISO8601). Use with start_time for explicit time range.',
            },
          },
          Required: ['instanceId', 'query'],
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            instanceId: { Type: 'string' },
            query: { Type: 'string' },
            filesSearched: { Type: 'integer' },
            filesWithMatches: { Type: 'integer' },
            totalMatches: { Type: 'integer' },
            results: { Type: 'array', Description: 'Array of {finding_id, file, fullKey, matchCount, matches[]}' },
            coverage_report: { Type: 'object' },
          },
        },
      },
      {
        Name: 'correlate',
        Description: 'Cross-file timeline correlation for incident analysis. Groups events by component, builds temporal clusters, and identifies potential root cause chains. CITATION: Cite finding_ids, confidence level, and any gaps reported.',
        InputSchema: {
          Type: 'object',
          Properties: {
            instanceId: {
              Type: 'string',
              Description: 'The EC2 instance ID to correlate events for',
            },
            timeWindow: {
              Type: 'integer',
              Description: 'Seconds around pivot event (default: 60)',
            },
            pivotEvent: {
              Type: 'string',
              Description: 'Event to correlate around (optional)',
            },
            components: {
              Type: 'array',
              Description: 'Components to include (optional)',
            },
            response_format: {
              Type: 'string',
              Description: '"concise" (default) or "detailed" — controls verbosity of timeline entries',
            },
            incident_time: {
              Type: 'string',
              Description: 'ISO8601 timestamp of the incident. Correlation will be restricted to +/- 5 minutes around this time. If omitted and no start_time/end_time, defaults to last 10 minutes.',
            },
            start_time: {
              Type: 'string',
              Description: 'Start of analysis window (ISO8601). Use with end_time for explicit time range.',
            },
            end_time: {
              Type: 'string',
              Description: 'End of analysis window (ISO8601). Use with start_time for explicit time range.',
            },
          },
          Required: ['instanceId'],
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            instanceId: { Type: 'string' },
            timeline: { Type: 'array' },
            byComponent: { Type: 'object' },
            correlations: { Type: 'array' },
            temporal_clusters: { Type: 'array', Description: 'Groups of events within timeWindow' },
            potential_root_cause_chain: { Type: 'array', Description: 'Ordered cause→effect chains' },
            confidence: { Type: 'string', Description: 'high|medium|low|none' },
            gaps: { Type: 'array', Description: 'Data quality issues that reduce confidence' },
            coverage_report: { Type: 'object' },
            window_start_utc: { Type: 'string', Description: 'UTC start of analysis window (ISO8601)' },
            window_end_utc: { Type: 'string', Description: 'UTC end of analysis window (ISO8601)' },
            resolution_reason: { Type: 'string', Description: 'How the time window was determined' },
          },
        },
      },
      {
        Name: 'artifact',
        Description: 'Get secure presigned URL for large artifacts. Use for files too large to return directly. CITATION: Cite the logKey and expiresAt timestamp.',
        InputSchema: {
          Type: 'object',
          Properties: {
            logKey: {
              Type: 'string',
              Description: 'The S3 key of the artifact',
            },
            expirationMinutes: {
              Type: 'integer',
              Description: 'URL expiration in minutes (default: 15, max: 60)',
            },
          },
          Required: ['logKey'],
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            logKey: { Type: 'string' },
            presignedUrl: { Type: 'string' },
            expiresAt: { Type: 'string' },
            sizeHuman: { Type: 'string' },
          },
        },
      },
      {
        Name: 'summarize',
        Description: 'Generate structured incident summary with automatic root cause triage. This is the PRIMARY analysis tool — call it after errors to get root cause, remediation steps, and confidence assessment in ONE call. Includes pod/node failure triage across 8 categories (Volume/CSI, Node Issues, CNI/Networking, iptables/conntrack, Scheduling, Image Pull, DNS, Secrets/Webhook). Pass finding_ids from errors tool. CITATION: Always cite the finding_ids that support each claim.',
        InputSchema: {
          Type: 'object',
          Properties: {
            instanceId: {
              Type: 'string',
              Description: 'The EC2 instance ID to summarize',
            },
            includeRecommendations: {
              Type: 'boolean',
              Description: 'Include remediation suggestions (default: true)',
            },
            finding_ids: {
              Type: 'array',
              Description: 'Optional list of finding_ids (F-001 format) from errors tool. When provided, summary is constrained to only these findings.',
            },
            incident_time: {
              Type: 'string',
              Description: 'ISO8601 timestamp of the incident. Summary analysis will be restricted to +/- 5 minutes around this time. If omitted and no start_time/end_time, defaults to last 10 minutes.',
            },
            start_time: {
              Type: 'string',
              Description: 'Start of analysis window (ISO8601). Use with end_time for explicit time range.',
            },
            end_time: {
              Type: 'string',
              Description: 'End of analysis window (ISO8601). Use with start_time for explicit time range.',
            },
          },
          Required: ['instanceId', 'finding_ids'],
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            instanceId: { Type: 'string' },
            grounded: { Type: 'boolean', Description: 'True if all claims backed by finding_ids' },
            unresolvedFindingIds: { Type: 'array', Description: 'finding_ids that could not be resolved' },
            criticalFindings: { Type: 'array' },
            highFindings: { Type: 'array' },
            affectedComponents: { Type: 'array' },
            recommendations: { Type: 'array' },
            confidence: { Type: 'string' },
            gaps: { Type: 'array' },
          },
        },
      },
      {
        Name: 'quick_triage',
        Description: 'FASTEST and MOST COMPLETE path to root cause. Combines validate + errors + triage in ONE call. Returns bundle status, error findings with log excerpts (topEvidence), root cause category, remediation steps, followup commands, and recommendedSOPs (auto-matched SOPs based on detected issues). The topEvidence field contains actual log line excerpts so you do NOT need to call search afterward. Only use read(logKey=...) if you need the full content of a specific file. When recommendedSOPs is present, call get_sop for each recommended SOP to get detailed remediation runbooks. CITATION: Cite instanceId, rootCause category, and confidence.',
        InputSchema: {
          Type: 'object',
          Properties: {
            instanceId: {
              Type: 'string',
              Description: 'The EC2 instance ID to triage',
            },
            severity: {
              Type: 'string',
              Description: 'Filter findings by severity: critical, high, medium, low, info, all (default: all)',
            },
            includeTriage: {
              Type: 'boolean',
              Description: 'Include pod/node failure triage analysis (default: true)',
            },
            incident_time: {
              Type: 'string',
              Description: 'ISO8601 timestamp of the incident. Triage will be restricted to +/- 5 minutes around this time. If omitted and no start_time/end_time, defaults to last 10 minutes.',
            },
            start_time: {
              Type: 'string',
              Description: 'Start of analysis window (ISO8601). Use with end_time for explicit time range.',
            },
            end_time: {
              Type: 'string',
              Description: 'End of analysis window (ISO8601). Use with start_time for explicit time range.',
            },
          },
          Required: ['instanceId'],
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            instanceId: { Type: 'string' },
            bundle: { Type: 'object', Description: 'Bundle validation: complete, fileCount, totalSize, missingPatterns' },
            errorSummary: { Type: 'object', Description: 'Counts by severity' },
            totalFindings: { Type: 'integer' },
            findings: { Type: 'array', Description: 'Top findings with finding_id, severity, pattern, file, count, sample' },
            topEvidence: { Type: 'array', Description: 'Top 15 log excerpts from critical/high/medium findings — actual log lines, no need to search' },
            rootCause: { Type: 'object', Description: 'Root cause: category, confidence, summary, detail' },
            remediation: { Type: 'array', Description: 'Immediate remediation steps' },
            followupCommands: { Type: 'array', Description: 'kubectl/AWS CLI commands to validate fix' },
            recommendations: { Type: 'array' },
            triage: { Type: 'object', Description: 'Full triage result with pod_states, node_conditions, evidence' },
            confidence: { Type: 'string', Description: 'high|medium|low' },
            window_start_utc: { Type: 'string', Description: 'UTC start of analysis window (ISO8601)' },
            window_end_utc: { Type: 'string', Description: 'UTC end of analysis window (ISO8601)' },
            resolution_reason: { Type: 'string', Description: 'How the time window was determined' },
            recommendedSOPs: { Type: 'array', Description: 'Auto-matched SOPs based on detected issues. Each entry has sopName, relevanceScore, matchedKeywords, reason. Call get_sop(sopName) for full remediation runbook.' },
            nextStep: { Type: 'string' },
          },
        },
      },
      {
        Name: 'history',
        Description: 'List historical log collections for audit and comparison. Supports cross-region listing. CITATION: Cite executionId and status for each entry.',
        InputSchema: {
          Type: 'object',
          Properties: {
            instanceId: {
              Type: 'string',
              Description: 'Filter by instance ID (optional)',
            },
            maxResults: {
              Type: 'integer',
              Description: 'Maximum results (default: 20, max: 50)',
            },
            status: {
              Type: 'string',
              Description: 'Filter by status: Success, Failed, InProgress (optional)',
            },
            region: {
              Type: 'string',
              Description: 'AWS region to list executions from (default: Lambda region). Specify to list executions from a different region.',
            },
          },
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            executions: { Type: 'array' },
            totalCount: { Type: 'integer' },
          },
        },
      },

      // =====================================================================
      // TIER 3: CLUSTER-LEVEL INTELLIGENCE
      // =====================================================================
      {
        Name: 'cluster_health',
        Description: 'Get a comprehensive health overview of an EKS cluster. Enumerates all nodes, checks SSM agent status, instance metadata (type, AZ, AMI, launch time), and flags unhealthy nodes. The entry point before diving into individual node investigation. CITATION: Cite clusterName, node counts, and any unhealthy node instanceIds.',
        InputSchema: {
          Type: 'object',
          Properties: {
            clusterName: {
              Type: 'string',
              Description: 'Name of the EKS cluster to inspect',
            },
            region: {
              Type: 'string',
              Description: 'AWS region of the cluster (auto-detected if omitted)',
            },
            includeSSMStatus: {
              Type: 'boolean',
              Description: 'Check SSM agent connectivity for each node (default: true)',
            },
          },
          Required: ['clusterName'],
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            clusterName: { Type: 'string' },
            totalNodes: { Type: 'integer' },
            healthyNodes: { Type: 'integer' },
            unhealthyNodes: { Type: 'integer' },
            nodes: { Type: 'array' },
          },
        },
      },
      {
        Name: 'compare_nodes',
        Description: 'Diff error findings and health status between two or more nodes. Surfaces what is unique to a failing node vs. common across all nodes. Saves tokens by returning a structured diff instead of raw summaries. CITATION: Cite instanceIds compared and unique findings per node.',
        InputSchema: {
          Type: 'object',
          Properties: {
            instanceIds: {
              Type: 'array',
              Description: 'List of 2+ EC2 instance IDs to compare (e.g., ["i-aaa", "i-bbb"])',
            },
            compareFields: {
              Type: 'string',
              Description: 'What to compare: "errors", "config", "all" (default: "all")',
            },
          },
          Required: ['instanceIds'],
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            commonFindings: { Type: 'array' },
            uniqueFindings: { Type: 'object', Description: 'Map of instanceId → unique findings' },
            insight: { Type: 'string' },
          },
        },
      },
      {
        Name: 'batch_collect',
        Description: 'Smart batch log collection with statistical sampling. Triages all nodes in a cluster, groups unhealthy nodes into buckets by failure signature, and collects from representative samples. Handles 1000+ node clusters efficiently. Use dryRun to preview before collecting. CITATION: Cite batchId, node count, and sampling strategy used.',
        InputSchema: {
          Type: 'object',
          Properties: {
            clusterName: {
              Type: 'string',
              Description: 'Name of the EKS cluster',
            },
            region: {
              Type: 'string',
              Description: 'AWS region of the cluster (auto-detected if omitted)',
            },
            filter: {
              Type: 'string',
              Description: 'Node filter: "all", "unhealthy", "notready" (default: "unhealthy")',
            },
            strategy: {
              Type: 'string',
              Description: '"sample" for smart sampling or "all" to collect from every filtered node (default: "sample")',
            },
            samplesPerBucket: {
              Type: 'integer',
              Description: 'Nodes to sample per failure bucket (default: 3, max: 5)',
            },
            maxTotalCollections: {
              Type: 'integer',
              Description: 'Hard cap on total collections (default: 15, max: 15)',
            },
            groupBy: {
              Type: 'string',
              Description: 'Grouping strategy: "auto", "az", "nodegroup", "instance-type", "ami" (default: "auto")',
            },
            dryRun: {
              Type: 'boolean',
              Description: 'Preview which nodes would be collected without starting (default: false)',
            },
          },
          Required: ['clusterName'],
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            batchId: { Type: 'string' },
            executions: { Type: 'array' },
            totalNodes: { Type: 'integer' },
            sampledNodes: { Type: 'integer' },
            strategy: { Type: 'string' },
            task: {
              Type: 'object',
              Description: 'Async task envelope — poll with batch_status using batchId',
              Properties: {
                taskId: { Type: 'string', Description: 'Same as batchId' },
                state: { Type: 'string', Description: 'running|completed|failed' },
                message: { Type: 'string' },
                progress: { Type: 'integer', Description: '0-100 percent' },
              },
            },
          },
        },
      },
      {
        Name: 'batch_status',
        Description: 'Poll status of multiple log collections at once. Returns consolidated view with allComplete boolean. Use after batch_collect to wait for all collections to finish before running analysis tools. CITATION: Cite allComplete status and any failed executionIds.',
        InputSchema: {
          Type: 'object',
          Properties: {
            executionIds: {
              Type: 'array',
              Description: 'List of SSM execution IDs to poll (from batch_collect response)',
            },
            batchId: {
              Type: 'string',
              Description: 'Batch ID from batch_collect (alternative to executionIds — loads execution IDs automatically)',
            },
          },
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            allComplete: { Type: 'boolean' },
            executions: { Type: 'array' },
            successCount: { Type: 'integer' },
            failedCount: { Type: 'integer' },
            inProgressCount: { Type: 'integer' },
          },
        },
      },
      {
        Name: 'network_diagnostics',
        Description: 'Extract and structure ALL networking info from collected log bundles in ONE call. Parses iptables rules, CNI config/env vars, route tables, DNS resolution, ENI attachment status, VPC CNI (aws-node/ipamd) logs, and kube-proxy mode/conntrack/IPVS status. Returns structured data with an eksNetworkingContext section containing guardrails to prevent misinterpretation, plus recommendedSOPs (auto-matched SOPs based on detected networking issues). When recommendedSOPs is present, call get_sop for each recommended SOP to get detailed remediation runbooks. IMPORTANT EKS NETWORKING RULES: (1) Empty host route table / no default gateway is NORMAL on multi-ENI EKS nodes — secondary ENIs handle pod traffic via policy routing and SNAT. (2) Missing SNAT/MASQUERADE in iptables is EXPECTED when AWS_VPC_K8S_CNI_EXTERNALSNAT=true — NAT gateway handles egress. (3) iptables FORWARD policy DROP breaks pod networking on custom AMIs. (4) Transient "no available IP" after pod deletion is normal — IP_COOLDOWN_PERIOD (default 30s) cache. (5) Pods with hostNetwork=true use node IP directly, no SNAT. (6) nm-cloud-setup (routing table 30200/30400) is INCOMPATIBLE with VPC CNI. (7) ENABLE_PREFIX_DELEGATION changes ENI slot behavior (/28 = 16 IPs per slot). (8) ENABLE_POD_ENI enables trunk ENI for security groups per pod — extra ENIs are expected. (9) NETWORK_POLICY_ENFORCING_MODE=strict means default deny for new pods. (10) systemd-udev MACAddressPolicy=persistent breaks veth MAC on Ubuntu 22.04+. (11) kube-proxy IPVS mode: KUBE-SVC iptables chains will NOT exist — use "ipvsadm -L" instead. Requires ip_vs kernel modules. (12) kube-proxy nftables mode: rules NOT visible via iptables-save — use "nft list ruleset". (13) "nf_conntrack: table full, dropping packet" = conntrack exhaustion — increase conntrack.min in kube-proxy-config ConfigMap. Each entry ~300 bytes. (14) kube-proxy version must be within 1 minor version of cluster control plane. (15) On RHEL 8.6+ (nftables-based OS), iptables mode kube-proxy may not work — use IPVS mode. Always read eksNetworkingContext.guardrails before concluding on any issue. CITATION: Cite instanceId and each section analyzed.',
        InputSchema: {
          Type: 'object',
          Properties: {
            instanceId: {
              Type: 'string',
              Description: 'The EC2 instance ID to analyze networking for',
            },
            sections: {
              Type: 'string',
              Description: 'Comma-separated sections: "iptables,cni,routes,dns,eni,ipamd,kube_proxy" or "all" (default: "all")',
            },
          },
          Required: ['instanceId'],
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            instanceId: { Type: 'string' },
            sections: { Type: 'object', Description: 'Map of section name → structured data' },
            eksNetworkingContext: { Type: 'object', Description: 'EKS-specific guardrails and context. MUST read guardrails array before diagnosing any networking issue. Contains cross-referenced findings (e.g., externalSnat, customNetworking, prefixDelegation, kubeProxyMode flags).' },
            assessment: { Type: 'string' },
            issues: { Type: 'array' },
            confidence: { Type: 'string', Description: 'high|medium|low|none' },
            gaps: { Type: 'array', Description: 'Data quality issues that reduce confidence' },
            recommendedSOPs: { Type: 'array', Description: 'Auto-matched SOPs based on detected networking issues. Each entry has sopName, relevanceScore, matchedKeywords, reason. Call get_sop(sopName) for full remediation runbook.' },
          },
        },
      },
      {
        Name: 'storage_diagnostics',
        Description: 'Extract and structure ALL storage/volume/CSI info from collected log bundles in ONE call. Parses kubelet volume mount errors (FailedMount, FailedAttachVolume, Multi-Attach), EBS CSI driver logs (controller + node), EFS CSI driver logs, PV/PVC/StorageClass status, and instance EBS attachment capacity. Returns structured data with an eksStorageContext section containing guardrails to prevent misinterpretation, plus recommendedSOPs (auto-matched SOPs based on detected storage issues). When recommendedSOPs is present, call get_sop for each recommended SOP to get detailed remediation runbooks. IMPORTANT EKS STORAGE RULES: (1) Multi-Attach error with ~6 minute delay after pod termination is NORMAL K8s behavior (maxWaitForUnmountDuration), NOT a CSI bug — check Node.Status.VolumesInUse. (2) EBS attachment slots are SHARED with ENIs on pre-Gen7 instances (m5/c5/r5) — VPC CNI ENIs consume EBS slots. Fix: prefix delegation, --reserved-volume-attachments, or Gen7+. (3) IMDSv2 hop limit must be >=2 for containerized CSI drivers. (4) ebs.csi.aws.com/agent-not-ready taint prevents scheduling before CSI is ready. (5) XFS "wrong fs type, bad superblock" on AL2 with newer xfsprogs — fix: --legacy-xfs=true. (6) EFS PV/PVC capacity is MEANINGLESS — EFS is elastic, capacity field is required by K8s but not enforced. (7) EFS dynamic provisioning = access points (up to 1000 per FS). EFS file system must be pre-created. (8) StorageClass kubernetes.io/aws-ebs uses deprecated in-tree driver — CSI migration translates at runtime. (9) EC2 API throttling from CSI sidecars with high --worker-threads affects ALL instances in account/region. (10) Network policies in strict mode can block CSI driver communication (EC2 API 443, NFS 2049). (11) For cross-VPC EFS mounts, botocore must be installed for DNS resolution fallback. Always read eksStorageContext.guardrails before concluding on any issue. CITATION: Cite instanceId and each section analyzed.',
        InputSchema: {
          Type: 'object',
          Properties: {
            instanceId: {
              Type: 'string',
              Description: 'The EC2 instance ID to analyze storage/volumes for',
            },
            sections: {
              Type: 'string',
              Description: 'Comma-separated sections: "kubelet,ebs_csi,efs_csi,pv_pvc,instance" or "all" (default: "all")',
            },
          },
          Required: ['instanceId'],
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            instanceId: { Type: 'string' },
            sections: { Type: 'object', Description: 'Map of section name → structured data' },
            eksStorageContext: { Type: 'object', Description: 'EKS-specific storage guardrails and context. MUST read guardrails array before diagnosing any storage/volume/CSI issue. Contains cross-referenced findings (e.g., instance type, ENI count, CSINode allocatable).' },
            assessment: { Type: 'string' },
            issues: { Type: 'array' },
            confidence: { Type: 'string', Description: 'high|medium|low|none' },
            gaps: { Type: 'array', Description: 'Data quality issues that reduce confidence' },
            recommendedSOPs: { Type: 'array', Description: 'Auto-matched SOPs based on detected storage issues. Each entry has sopName, relevanceScore, matchedKeywords, reason. Call get_sop(sopName) for full remediation runbook.' },
          },
        },
      },
      {
        Name: 'tcpdump_capture',
        Description: 'Run tcpdump on an EKS worker node via SSM Run Command for a specified duration (default 2 minutes), then upload the pcap file to S3. Supports capturing inside a pod/container network namespace — provide podName (auto-resolves PID via crictl/docker) or containerPid (raw PID). For K8s DNS debugging: tcpdump_capture(instanceId, podName="coredns-xxx", podNamespace="kube-system", filter="udp port 53"). Returns immediately with a commandId for async polling. Call again with commandId to check status. CITATION: Cite commandId, instanceId, and s3Key.',
        InputSchema: {
          Type: 'object',
          Properties: {
            instanceId: {
              Type: 'string',
              Description: 'The EC2 instance ID of the EKS worker node (e.g., i-0123456789abcdef0)',
            },
            durationSeconds: {
              Type: 'integer',
              Description: 'Capture duration in seconds (default: 120, min: 10, max: 300)',
            },
            interface: {
              Type: 'string',
              Description: 'Network interface to capture on (default: "any"). Use "eth0", "eni+", etc.',
            },
            filter: {
              Type: 'string',
              Description: 'BPF filter expression (e.g., "port 443", "host 10.0.0.1 and port 80", "udp port 53")',
            },
            podName: {
              Type: 'string',
              Description: 'Kubernetes pod name to capture from (e.g., "coredns-5d78c9869d-abc12"). Auto-resolves to container PID via crictl/docker on the worker node. Pod must be running on the specified instanceId.',
            },
            podNamespace: {
              Type: 'string',
              Description: 'Kubernetes namespace of the pod (default: "default"). Use "kube-system" for CoreDNS, "amazon-vpc-cni" for VPC CNI pods, etc.',
            },
            containerPid: {
              Type: 'string',
              Description: 'Raw container PID for nsenter (alternative to podName). Use when you already know the PID from "ps ax | grep <process>" on the worker node.',
            },
            commandId: {
              Type: 'string',
              Description: 'SSM Command ID from a previous tcpdump_capture call — pass this to poll status',
            },
            region: {
              Type: 'string',
              Description: 'AWS region where the instance runs (optional, auto-detected)',
            },
          },
          Required: ['instanceId'],
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            commandId: { Type: 'string', Description: 'SSM Run Command ID for polling' },
            instanceId: { Type: 'string' },
            status: { Type: 'string', Description: 'in_progress | completed | failed' },
            s3Key: { Type: 'string', Description: 'S3 key of the uploaded pcap file' },
            s3Bucket: { Type: 'string' },
            fileSizeBytes: { Type: 'integer' },
            fileSizeHuman: { Type: 'string' },
            presignedUrl: { Type: 'string', Description: 'Presigned download URL (1 hour expiry)' },
            task: {
              Type: 'object',
              Description: 'Async task envelope for polling',
              Properties: {
                taskId: { Type: 'string', Description: 'Same as commandId' },
                state: { Type: 'string', Description: 'running|completed|failed' },
                message: { Type: 'string' },
                progress: { Type: 'integer', Description: '0-100 percent' },
              },
            },
          },
        },
      },
      {
        Name: 'tcpdump_analyze',
        Description: 'Read and analyze a completed tcpdump capture from S3. Returns decoded packet text (human-readable), protocol statistics (TCP/UDP/ICMP breakdown), top source/destination IPs, and anomaly detection (high RST rate, retransmissions, SYN floods). Use after tcpdump_capture completes. Supports text filtering to search for specific IPs, ports, or flags in the decoded output. CITATION: Cite commandId, packet count, and any anomalies found.',
        InputSchema: {
          Type: 'object',
          Properties: {
            instanceId: {
              Type: 'string',
              Description: 'The EC2 instance ID (e.g., i-0123456789abcdef0)',
            },
            commandId: {
              Type: 'string',
              Description: 'SSM Command ID from tcpdump_capture. If omitted, returns the latest capture for this instance.',
            },
            section: {
              Type: 'string',
              Description: '"summary" (decoded packets), "stats" (protocol breakdown + anomalies), "all" (default: "all")',
            },
            maxPackets: {
              Type: 'integer',
              Description: 'Max decoded packet lines to return (default: 500, max: 3000)',
            },
            filter: {
              Type: 'string',
              Description: 'Text filter on decoded lines (e.g., "SYN", "RST", "10.0.0.5", "port 443")',
            },
          },
          Required: ['instanceId'],
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            instanceId: { Type: 'string' },
            commandId: { Type: 'string' },
            captureInfo: { Type: 'object', Description: 'interface, filter, duration, startedAt' },
            statistics: { Type: 'object', Description: 'totalPackets, protocols (tcp/udp/icmp/arp), ports (dns/http/https), tcpFlags (syn/rst), topSourceIPs, topDestinationIPs' },
            anomalies: { Type: 'array', Description: 'Detected anomalies: high_rst_rate, retransmissions, syn_rst_ratio, high_icmp' },
            decodedPackets: { Type: 'object', Description: 'lines (array of decoded packet strings), totalPackets, returnedPackets, truncated, filter' },
            pcapDownloadUrl: { Type: 'string', Description: 'Presigned URL to download the raw pcap file (1 hour expiry)' },
            s3KeyPcap: { Type: 'string' },
            s3KeyTxt: { Type: 'string' },
            s3KeyStats: { Type: 'string' },
          },
        },
      },
      // =====================================================================
      // SOP MANAGEMENT TOOLS
      // =====================================================================
      {
        Name: 'list_sops',
        Description: 'List all available Standard Operating Procedures (SOPs) in the S3 bucket. Returns name, size, and last modified date for each SOP. IMPORTANT: quick_triage, network_diagnostics, and storage_diagnostics automatically return recommendedSOPs based on detected issues — check those first before browsing the full SOP list.',
        InputSchema: {
          Type: 'object',
          Properties: {},
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            sops: { Type: 'array', Description: 'Array of {name, size, lastModified}' },
            count: { Type: 'integer' },
            bucket: { Type: 'string' },
          },
        },
      },
      {
        Name: 'get_sop',
        Description: 'Get a specific Standard Operating Procedure (SOP) by name. Returns the full content of the SOP file. Called automatically when quick_triage, network_diagnostics, or storage_diagnostics return recommendedSOPs — use the sopName from recommendedSOPs directly. Can also be used after list_sops to retrieve any SOP manually.',
        InputSchema: {
          Type: 'object',
          Properties: {
            sopName: {
              Type: 'string',
              Description: 'The name/key of the SOP file to retrieve (e.g., "runbooks/pod-crashloop.md")',
            },
          },
          Required: ['sopName'],
        },
        OutputSchema: {
          Type: 'object',
          Properties: {
            sop: { Type: 'object', Description: '{name, content, size, lastModified, contentType}' },
          },
        },
      },
      // =====================================================================

    ];
  }

  /**
   * Returns the Unzip Lambda function code with findings indexer trigger
   */
  private getUnzipLambdaCode(): string {
    return `
import json
import boto3
import zipfile
import tarfile
import io
import os
from datetime import datetime

s3_client = boto3.client('s3')
lambda_client = boto3.client('lambda')

FINDINGS_INDEXER_FUNCTION = os.environ.get('FINDINGS_INDEXER_FUNCTION', '')

def get_content_type(file_name):
    if file_name.endswith('.log') or file_name.endswith('.txt'):
        return 'text/plain'
    elif file_name.endswith('.json'):
        return 'application/json'
    elif file_name.endswith('.yaml') or file_name.endswith('.yml'):
        return 'text/yaml'
    return 'application/octet-stream'

def sanitize_archive_path(file_name):
    """Sanitize archive member path to prevent zip-slip path traversal."""
    # Normalize and reject any path with .. components
    normalized = os.path.normpath(file_name)
    if normalized.startswith('..') or '/../' in normalized or normalized.startswith('/'):
        print(f"SECURITY: Skipping suspicious archive path: {file_name}")
        return None
    # Strip leading ./ if present
    return normalized.lstrip('./')

def extract_zip(bucket, key, content):
    base_path = key[:-4]
    extract_prefix = f"{base_path}/extracted/"
    extracted_files = []
    with zipfile.ZipFile(io.BytesIO(content), 'r') as zip_ref:
        for file_info in zip_ref.infolist():
            if file_info.is_dir():
                continue
            file_name = sanitize_archive_path(file_info.filename)
            if file_name is None:
                continue
            file_content = zip_ref.read(file_info.filename)
            extract_key = f"{extract_prefix}{file_name}"
            s3_client.put_object(
                Bucket=bucket, 
                Key=extract_key, 
                Body=file_content, 
                ContentType=get_content_type(file_name)
            )
            extracted_files.append(extract_key)
            print(f"Extracted: {extract_key}")
    return extracted_files, extract_prefix

def extract_targz(bucket, key, content):
    if key.endswith('.tar.gz'):
        base_path = key[:-7]
    else:
        base_path = key[:-4]
    extract_prefix = f"{base_path}/extracted/"
    extracted_files = []
    with tarfile.open(fileobj=io.BytesIO(content), mode='r:gz') as tar_ref:
        for member in tar_ref.getmembers():
            if not member.isfile():
                continue
            file_name = sanitize_archive_path(member.name)
            if file_name is None:
                continue
            file_obj = tar_ref.extractfile(member)
            if file_obj is None:
                continue
            file_content = file_obj.read()
            extract_key = f"{extract_prefix}{file_name}"
            s3_client.put_object(
                Bucket=bucket, 
                Key=extract_key, 
                Body=file_content, 
                ContentType=get_content_type(file_name)
            )
            extracted_files.append(extract_key)
            print(f"Extracted: {extract_key}")
    return extracted_files, extract_prefix

def trigger_findings_indexer(bucket, prefix, file_count):
    """Trigger the findings indexer Lambda after extraction."""
    if not FINDINGS_INDEXER_FUNCTION:
        print("No findings indexer function configured, skipping")
        return
    
    try:
        lambda_client.invoke(
            FunctionName=FINDINGS_INDEXER_FUNCTION,
            InvocationType='Event',  # Async invocation
            Payload=json.dumps({
                'bucket': bucket,
                'prefix': prefix,
                'fileCount': file_count
            })
        )
        print(f"Triggered findings indexer for prefix: {prefix}")
    except Exception as e:
        print(f"Failed to trigger findings indexer: {str(e)}")

def generate_manifest(bucket, prefix, extracted_files, archive_key, archive_size):
    """Generate manifest.json with authoritative file inventory after extraction."""
    try:
        file_details = []
        for file_key in extracted_files:
            try:
                head = s3_client.head_object(Bucket=bucket, Key=file_key)
                relative_path = file_key.split('/extracted/')[-1] if '/extracted/' in file_key else file_key
                file_details.append({
                    'key': relative_path,
                    'fullKey': file_key,
                    'size': head['ContentLength'],
                    'contentType': head.get('ContentType', 'application/octet-stream'),
                })
            except Exception:
                file_details.append({
                    'key': file_key.split('/extracted/')[-1] if '/extracted/' in file_key else file_key,
                    'fullKey': file_key,
                    'size': 0,
                    'contentType': 'unknown',
                })
        
        manifest = {
            'version': 2,
            'generatedAt': datetime.utcnow().isoformat() + 'Z',
            'archiveKey': archive_key,
            'archiveSize': archive_size,
            'totalFiles': len(extracted_files),
            'totalSize': sum(f['size'] for f in file_details),
            'files': file_details,
        }
        
        manifest_key = f"{prefix}manifest.json"
        s3_client.put_object(
            Bucket=bucket,
            Key=manifest_key,
            Body=json.dumps(manifest, default=str),
            ContentType='application/json'
        )
        print(f"Wrote manifest.json to {manifest_key} ({len(extracted_files)} files)")
    except Exception as e:
        print(f"Warning: Failed to generate manifest.json: {str(e)}")

def lambda_handler(event, context):
    print(f"Received event: {json.dumps(event)}")
    
    for record in event.get('Records', []):
        bucket = record['s3']['bucket']['name']
        key = record['s3']['object']['key']
        
        if '/extracted/' in key:
            print(f"Skipping already extracted file: {key}")
            continue
        
        is_zip = key.lower().endswith('.zip')
        is_targz = key.lower().endswith('.tar.gz') or key.lower().endswith('.tgz')
        
        if not is_zip and not is_targz:
            print(f"Skipping unsupported file type: {key}")
            continue
        
        print(f"Processing archive: s3://{bucket}/{key}")
        
        try:
            response = s3_client.get_object(Bucket=bucket, Key=key)
            content = response['Body'].read()
            
            if is_zip:
                extracted_files, prefix = extract_zip(bucket, key, content)
            else:
                extracted_files, prefix = extract_targz(bucket, key, content)
            
            print(f"Successfully extracted {len(extracted_files)} files from {key}")
            
            # Generate manifest.json with authoritative file inventory
            generate_manifest(bucket, prefix, extracted_files, key, len(content))
            
            # Trigger findings indexer
            trigger_findings_indexer(bucket, prefix, len(extracted_files))
            
        except (zipfile.BadZipFile, tarfile.TarError) as e:
            print(f"Error: {key} is not a valid archive: {str(e)}")
        except Exception as e:
            print(f"Error processing {key}: {str(e)}")
            raise e
    
    return {
        'statusCode': 200, 
        'body': json.dumps({'message': 'Archive extraction complete'})
    }
`;
  }

  /**
   * Returns the Findings Indexer Lambda function code
   */
  private getFindingsIndexerCode(): string {
    return `
import json
import boto3
import os
import re
from datetime import datetime

s3_client = boto3.client('s3')
LOGS_BUCKET = os.environ['LOGS_BUCKET_NAME']

# Files worth scanning (log files with actual runtime output).
# Skip config dumps, static system info, metrics, and package lists
# which produce massive false positives.
SCANNABLE_FILE_PATTERNS = [
    r'kubelet[/.]',
    r'containerd[/-]log',
    r'docker[/.]',
    r'dmesg',
    r'messages$',
    r'secure$',
    r'cloud-init.*\\.log',
    r'ipamd\\.log',
    r'aws-routed-eni/.*\\.log',
    r'kube-proxy.*\\.log',
    r'coredns.*\\.log',
    r'var_log/.*\\.log',
    r'nodeadm',
    r'sandbox-image',
]
SCANNABLE_REGEXES = [re.compile(p, re.IGNORECASE) for p in SCANNABLE_FILE_PATTERNS]

# Files to always skip - these contain config/static data, not errors
SKIP_FILE_PATTERNS = [
    r'sysctl',
    r'ethtool',
    r'ifconfig',
    r'conntrack\\.txt',
    r'pkglist',
    r'ps\\.txt',
    r'ps-threads',
    r'top\\.txt',
    r'allprocstat',
    r'mounts\\.txt',
    r'containerd-config\\.txt',
    r'containerd-plugins\\.txt',
    r'containerd-containers\\.txt',
    r'containerd-images\\.txt',
    r'containerd-namespaces\\.txt',
    r'containerd-tasks\\.txt',
    r'containerd-version\\.txt',
    r'metrics\\.json',
    r'modinfo',
    r'iptables',
    r'\\.json$',
    r'\\.yaml$',
    r'\\.yml$',
    r'\\.conf$',
    r'\\.toml$',
    r'\\.service$',
]
SKIP_REGEXES = [re.compile(p, re.IGNORECASE) for p in SKIP_FILE_PATTERNS]

# Precise error patterns - these require actual error context, not config values.
# Each pattern is a tuple of (regex, description) for better reporting.
ERROR_PATTERNS = {
    'critical': [
        (r'kernel panic', 'kernel panic'),
        (r'BUG:.*', 'kernel bug'),
        (r'watchdog: BUG: soft lockup', 'soft lockup'),
        (r'invoked oom-killer', 'OOM killer invoked'),
        (r'Out of memory: Kill', 'OOM kill'),
        (r'Killed process \\d+.*total-vm', 'OOM kill with details'),
        (r'Memory cgroup out of memory.*process', 'cgroup OOM'),
        (r'segfault at', 'segfault'),
        (r'PLEG is not healthy', 'PLEG unhealthy'),
        (r'failed to run Kubelet:', 'kubelet launch failure'),
        (r'Unit kubelet.*entered failed state', 'kubelet failed'),
        (r'Node became not ready', 'node NotReady'),
        (r'OCI runtime create failed:', 'container runtime failure'),
        (r'no networks found in /etc/cni/net\\.d', 'CNI missing'),
        (r'Container runtime network not ready', 'runtime network not ready'),
        (r'InsufficientFreeAddressesInSubnet', 'IP exhaustion'),
        (r'Unable to register node', 'node registration failed'),
        (r'failed to register node', 'node registration failed'),
        (r'Instances failed to join', 'cluster join failure'),
        (r'certificate has expired', 'expired certificate'),
        (r'x509: certificate', 'certificate error'),
        (r'Unauthorized', 'auth failure'),
        (r'nodeadm.*(?:failed|error)', 'nodeadm failure'),
        (r'Failed to pull image', 'image pull failure'),
        (r'CrashLoopBackOff', 'crash loop'),
        (r'fork/exec.*resource temporarily unavailable', 'PID exhaustion'),
    ],
    'warning': [
        (r'(?:Readiness|Liveness|Startup) probe.*failed', 'probe failure'),
        (r'Back-off restarting failed container', 'container restart backoff'),
        (r'ImagePullBackOff', 'image pull backoff'),
        (r'ErrImagePull', 'image pull error'),
        (r'FailedScheduling', 'scheduling failure'),
        (r'FailedMount', 'mount failure'),
        (r'FailedAttachVolume', 'volume attach failure'),
        (r'Insufficient cpu', 'insufficient CPU'),
        (r'Insufficient memory', 'insufficient memory'),
        (r'OOMKilled', 'container OOMKilled'),
        (r'Evicted', 'pod evicted'),
        (r'NetworkNotReady', 'network not ready'),
        (r'dial tcp.*connection refused', 'connection refused'),
        (r'dial tcp.*i/o timeout', 'connection timeout'),
        (r'TLS handshake timeout', 'TLS timeout'),
        (r'no such host', 'DNS resolution failure'),
        (r'NXDOMAIN', 'DNS NXDOMAIN'),
        (r'SERVFAIL', 'DNS SERVFAIL'),
        (r'is not authorized to perform', 'IAM permission denied'),
        (r'systemd.*Failed to start', 'service start failure'),
        (r'nfs: server.*not responding', 'NFS not responding'),
    ],
    'info': [
        (r'(?i)level=(?:warn|warning)', 'log-level warning'),
        (r'DEPRECATION:', 'deprecation notice'),
        (r'context deadline exceeded', 'context deadline'),
        (r'net_ratelimit:.*callbacks suppressed', 'kernel rate limiting'),
    ],
}

# Pre-compile all patterns
COMPILED_PATTERNS = {}
for _sev, _pats in ERROR_PATTERNS.items():
    COMPILED_PATTERNS[_sev] = []
    for _pat, _desc in _pats:
        try:
            COMPILED_PATTERNS[_sev].append((re.compile(_pat), _desc))
        except re.error:
            pass

def is_scannable(filename):
    """Check if a file is worth scanning (actual log output, not config/static data)."""
    # First check skip list
    for skip_re in SKIP_REGEXES:
        if skip_re.search(filename):
            return False
    # Then check if it matches known log patterns
    for scan_re in SCANNABLE_REGEXES:
        if scan_re.search(filename):
            return True
    # Default: scan if it looks like a log file
    if filename.endswith('.log') or filename.endswith('.txt'):
        # Only scan files in directories that typically contain logs
        log_dirs = ['kubelet', 'var_log', 'kernel', 'system/messages']
        return any(d in filename for d in log_dirs)
    return False

def lambda_handler(event, context):
    print(f"Received event: {json.dumps(event)}")
    
    bucket = event.get('bucket', LOGS_BUCKET)
    prefix = event.get('prefix', '')
    
    if not prefix:
        print("No prefix provided, skipping")
        return {'statusCode': 400, 'body': 'No prefix provided'}
    
    try:
        # List all extracted files
        files_to_scan = []
        files_skipped = 0
        paginator = s3_client.get_paginator('list_objects_v2')
        
        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
            for obj in page.get('Contents', []):
                key = obj['Key']
                # Skip non-text files
                if any(key.endswith(ext) for ext in ['.tar.gz', '.zip', '.gz', '.bin', '.so']):
                    continue
                # Skip very large files
                if obj['Size'] > 5242880:  # 5MB
                    continue
                # Only scan actual log files, not config/static data
                filename = key.split('/extracted/')[-1] if '/extracted/' in key else key
                if not is_scannable(filename):
                    files_skipped += 1
                    continue
                files_to_scan.append({
                    'key': key,
                    'size': obj['Size']
                })
        
        print(f"Scanning {len(files_to_scan)} log files ({files_skipped} non-log files skipped)")
        
        # Scan files
        all_findings = []
        for file_info in files_to_scan[:200]:  # Limit to prevent timeout
            findings = scan_file(bucket, file_info['key'])
            all_findings.extend(findings)
        
        # Deduplicate
        deduplicated = deduplicate_findings(all_findings)
        
        # False positive suppression (Phase 2.6)
        FALSE_POSITIVE_SUPPRESSIONS = [
            ('NXDOMAIN', r'health[-.]?check|readiness|liveness', 'Health check DNS lookup'),
            ('OOMKilled', r'stress[-.]?test|load[-.]?test|chaos', 'Stress test pod'),
            ('connection refused', r'127\\.0\\.0\\.1:10256.*healthz', 'kube-proxy local healthz'),
            ('TLS handshake', r'kube-probe|health[-.]?check', 'Probe TLS handshake'),
        ]
        suppressed_count = 0
        filtered = []
        for f in deduplicated:
            suppressed = False
            pat = f.get('pattern', '')
            ctx = f.get('sample', '')
            for err_pat, fp_regex, reason in FALSE_POSITIVE_SUPPRESSIONS:
                if err_pat.lower() in pat.lower():
                    if re.search(fp_regex, ctx, re.IGNORECASE):
                        suppressed = True
                        suppressed_count += 1
                        break
            if not suppressed:
                filtered.append(f)
        deduplicated = filtered
        
        # Assign finding_ids
        for idx, finding in enumerate(deduplicated):
            finding['finding_id'] = f"F-{idx + 1:03d}"
            # Add evidence wrapper (Phase 2.2)
            finding['evidence'] = {
                'source_file': finding.get('file', ''),
                'full_key': finding.get('fullKey', ''),
                'excerpt': finding.get('sample', '')[:500],
                'line_range': {'start': finding.get('line', 0), 'end': finding.get('line', 0)},
            }
        
        # Multi-signal confirmation for critical findings (Phase 2.5)
        pattern_files = {}
        for f in deduplicated:
            p = f.get('pattern', '')
            if p not in pattern_files:
                pattern_files[p] = set()
            pattern_files[p].add(f.get('file', ''))
        for f in deduplicated:
            if f.get('severity') == 'critical':
                sources = list(pattern_files.get(f.get('pattern', ''), set()))
                f['confirmation'] = {
                    'signals': len(sources),
                    'confirmed': len(sources) >= 2,
                    'sources': sources[:5],
                }
                if len(sources) < 2:
                    f['severity_note'] = 'Single-source critical finding. Verify with additional log sources.'
        
        # Calculate summary with 5-level severity
        summary = {
            'critical': len([f for f in deduplicated if f.get('severity') == 'critical']),
            'high': len([f for f in deduplicated if f.get('severity') in ('warning', 'high')]),
            'medium': len([f for f in deduplicated if f.get('severity') == 'medium']),
            'low': len([f for f in deduplicated if f.get('severity') == 'low']),
            'info': len([f for f in deduplicated if f.get('severity') == 'info']),
        }
        
        # Build coverage block (Phase 2.2)
        total_files_in_bundle = len(files_to_scan) + files_skipped
        coverage = {
            'files_scanned': len(files_to_scan),
            'total_files': total_files_in_bundle,
            'coverage_pct': round(len(files_to_scan) / max(total_files_in_bundle, 1) * 100, 1),
            'files_skipped': files_skipped,
            'suppressed_false_positives': suppressed_count,
        }
        
        # Write index file (v2 schema)
        index_data = {
            'index_version': 'v2',
            'indexedAt': datetime.utcnow().isoformat(),
            'prefix': prefix,
            'coverage': coverage,
            'filesScanned': len(files_to_scan),
            'filesSkipped': files_skipped,
            'findings': deduplicated[:500],  # Limit stored findings
            'summary': summary,
        }
        
        index_key = f"{prefix}findings_index.json"
        s3_client.put_object(
            Bucket=bucket,
            Key=index_key,
            Body=json.dumps(index_data, default=str),
            ContentType='application/json'
        )
        
        print(f"Wrote findings index to {index_key}")
        print(f"Summary: {summary}")
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'Findings indexed successfully',
                'indexKey': index_key,
                'summary': summary
            })
        }
        
    except Exception as e:
        print(f"Error indexing findings: {str(e)}")
        return {'statusCode': 500, 'body': str(e)}

def scan_file(bucket, key):
    findings = []
    
    try:
        response = s3_client.get_object(
            Bucket=bucket,
            Key=key,
            Range='bytes=0-1048575'  # First 1MB
        )
        
        content = response['Body'].read()
        try:
            content_str = content.decode('utf-8')
        except:
            content_str = content.decode('latin-1', errors='ignore')
        
        filename = key.split('/extracted/')[-1] if '/extracted/' in key else key
        lines = content_str.split('\\n')[:5000]
        
        for severity, compiled_list in COMPILED_PATTERNS.items():
            for regex, description in compiled_list:
                match_count = 0
                for i, line in enumerate(lines):
                    if regex.search(line):
                        findings.append({
                            'file': filename,
                            'fullKey': key,
                            'severity': severity,
                            'pattern': description,
                            'line': i + 1,
                            'sample': line.strip()[:300]
                        })
                        match_count += 1
                        if match_count > 50 or len(findings) > 200:
                            break
                if len(findings) > 200:
                    return findings
    except Exception as e:
        print(f"Error scanning {key}: {str(e)}")
    
    return findings

def deduplicate_findings(findings):
    seen = {}
    
    for finding in findings:
        dedup_key = f"{finding.get('file')}:{finding.get('pattern')}"
        
        if dedup_key not in seen:
            seen[dedup_key] = {
                **finding,
                'count': 1,
                'lines': [finding.get('line')],
                'first_seen': finding.get('sample', '')[:50],
                'last_seen': finding.get('sample', '')[:50],
            }
        else:
            seen[dedup_key]['count'] += 1
            if len(seen[dedup_key]['lines']) < 10:
                seen[dedup_key]['lines'].append(finding.get('line'))
            seen[dedup_key]['last_seen'] = finding.get('sample', '')[:50]
    
    # Map old severity names to 5-level
    severity_map = {'warning': 'high'}
    for entry in seen.values():
        old_sev = entry.get('severity', 'info')
        if old_sev in severity_map:
            entry['severity'] = severity_map[old_sev]
    
    severity_order = {'critical': 0, 'high': 1, 'medium': 2, 'low': 3, 'info': 4}
    result = sorted(
        seen.values(),
        key=lambda x: (severity_order.get(x.get('severity'), 4), -x.get('count', 0))
    )
    
    return result
`;
  }

  /**
   * Returns the Client Secret Retriever Lambda function code
   */
  private getClientSecretRetrieverCode(): string {
    return `
import boto3
import json

def handler(event, context):
    try:
        if isinstance(event, str):
            event = json.loads(event)
        if 'Payload' in event:
            event = json.loads(event['Payload'])
        if event.get('RequestType') == 'Delete':
            return {'statusCode': 200, 'body': json.dumps({'ClientSecret': ''})}
        
        props = event.get('ResourceProperties', event)
        user_pool_id = props['UserPoolId']
        client_id = props['ClientId']
        
        cognito = boto3.client('cognito-idp')
        response = cognito.describe_user_pool_client(
            UserPoolId=user_pool_id, 
            ClientId=client_id
        )
        client_secret = response['UserPoolClient'].get('ClientSecret', '')
        
        return {
            'statusCode': 200, 
            'body': json.dumps({'ClientSecret': client_secret})
        }
    except Exception as e:
        print(f"Error: {str(e)}")
        return {'statusCode': 500, 'body': json.dumps({'Error': str(e)})}
`;
  }
}
